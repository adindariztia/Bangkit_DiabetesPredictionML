{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PIMA Dataset Overview",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adindariztia/Bangkit_DiabetesPredictionML/blob/Intan/PIMA_Dataset_Overview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4qeJ80ud2jC",
        "colab_type": "text"
      },
      "source": [
        "<h2> Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngTWoEBC7zrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZuqanp64xm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "import tensorflow as tf\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ratiCb5BLn",
        "colab_type": "text"
      },
      "source": [
        "<h2> Read The Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTPA0Rcf4dgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('diabetes.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01Bc0PsFjd2X",
        "colab_type": "text"
      },
      "source": [
        "# Preliminary Exploration with Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIIGebXs4_jj",
        "colab_type": "text"
      },
      "source": [
        "First, let's see how many rows and columns in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYjVS5o75MQf",
        "colab_type": "code",
        "outputId": "57176eb4-f7c2-47a1-cdc5-41b496b3ab96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtAh-5_3e2Qy",
        "colab_type": "text"
      },
      "source": [
        "Next, see the first rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2QXt0UB493u",
        "colab_type": "code",
        "outputId": "17a1f171-5b7a-4ed7-d492-ecf7d0d0dfbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs6Cu5c-fEcx",
        "colab_type": "text"
      },
      "source": [
        "There are 8 feature columns and 1 outcome. \n",
        "\n",
        "It's also good to know the distribution of each columns. So, let's see that by describe function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8_ZypMs5PhQ",
        "colab_type": "code",
        "outputId": "4798da0a-c72f-4f0d-aa5e-df0be432ca6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.845052</td>\n",
              "      <td>120.894531</td>\n",
              "      <td>69.105469</td>\n",
              "      <td>20.536458</td>\n",
              "      <td>79.799479</td>\n",
              "      <td>31.992578</td>\n",
              "      <td>0.471876</td>\n",
              "      <td>33.240885</td>\n",
              "      <td>0.348958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.369578</td>\n",
              "      <td>31.972618</td>\n",
              "      <td>19.355807</td>\n",
              "      <td>15.952218</td>\n",
              "      <td>115.244002</td>\n",
              "      <td>7.884160</td>\n",
              "      <td>0.331329</td>\n",
              "      <td>11.760232</td>\n",
              "      <td>0.476951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.078000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>27.300000</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>30.500000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>0.372500</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>140.250000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>127.250000</td>\n",
              "      <td>36.600000</td>\n",
              "      <td>0.626250</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>67.100000</td>\n",
              "      <td>2.420000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Pregnancies     Glucose  ...         Age     Outcome\n",
              "count   768.000000  768.000000  ...  768.000000  768.000000\n",
              "mean      3.845052  120.894531  ...   33.240885    0.348958\n",
              "std       3.369578   31.972618  ...   11.760232    0.476951\n",
              "min       0.000000    0.000000  ...   21.000000    0.000000\n",
              "25%       1.000000   99.000000  ...   24.000000    0.000000\n",
              "50%       3.000000  117.000000  ...   29.000000    0.000000\n",
              "75%       6.000000  140.250000  ...   41.000000    1.000000\n",
              "max      17.000000  199.000000  ...   81.000000    1.000000\n",
              "\n",
              "[8 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEIt2LSlgsmq",
        "colab_type": "text"
      },
      "source": [
        "Since this is a binary-output dataset, let's check whether the output is balanced or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvNzoFOvTkvM",
        "colab_type": "code",
        "outputId": "ed5b0940-4198-4d12-a11f-dc9211e46f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (\"{0:.0%}\".format(df.Outcome.mean()))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xSlnuQyj0pZ",
        "colab_type": "text"
      },
      "source": [
        "This dataset is not a balanced one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8ochja55XnV",
        "colab_type": "text"
      },
      "source": [
        "Create Correlation Matrix to Know Correlating Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy7c58ay5VDN",
        "colab_type": "code",
        "outputId": "3dfa89aa-8121-49d5-a73e-a938d784021f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        }
      },
      "source": [
        "corr = df.corr()\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "\n",
        "# Draw the heatmap with the mask and correct aspect ratio\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa5f19098d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtEAAAKBCAYAAACRR/fwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdebhdZXnH/e8vYRQQGSxaBoMSRHBA\nRq2oCAhYLWjFikWBisYJO1j6itUXFbWi2PpWxSEqgggFcUwtNVAEFQRJgDAqEgE11AliaVVkyv3+\nsdfRzeGc5KycdbLP2fl+rmtfWftZz1rrXif543ee3HvtVBWSJEmSJm7WoAuQJEmSZhpDtCRJktSS\nIVqSJElqyRAtSZIktWSIliRJkloyREuSJEktrTPoAjQUfE6iJEnTUwZdwLByJVqSJElqyRAtSZIk\ntWSIliRJkloyREuSJEktGaIlSZKklgzRkiRJUkuGaEmSJKklQ7QkSZLUkiFakiRJaskQLUmSJLVk\niJYkSZJaMkRLkiRJLRmiJUmSpJYM0ZIkSVJLhmhJkiSpJUO0JEmS1JIhWpIkSWrJEC1JkiS1ZIiW\nJEmSWjJES5IkSS0ZoiVJkqSWDNGSJElSS4bo1ZTkgSRLklyf5NwkDxt0TROR5JAkxw+6DkmSpJks\nVTXoGmakJL+uqo2b7TOBK6vqX/r2r1NV9w+swDXLf0SSJE1PGXQBw8qV6G58G9ghyb5Jvp1kAXBj\nktlJTk6yKMm1SV4DkGRWko8m+X6SC5Kcl+SwZt9tSd6Z5Kok1yXZqRnfK8llSa5O8p0kj2/Gj07y\npSRfT3JzkvePFJXk4OY81yS5sG/+R5rtRyb5YlPfoiTPaMaf3ayyL2mut8ma/GFKkiRNd+sMuoCZ\nLsk6wPOArzdDuwFPrKpbk8wD7qqqPZOsD1ya5Hxgd2AOsDPwR8D3gFP7TntHVe2W5PXAccCrgO8D\nz6yq+5McAPwT8OJm/q7AU4F7gJuSfBj4HfBJ4FlNLZuPUf6/Ah+sqkuSbAcsBJ7QXPMNVXVpko2b\nc0mSJKnhSvTq2zDJEmAx8GPg0834FVV1a7N9IHBkM++7wBbAXGAf4NyqWlFVPwMuGnXuLzV/Xkkv\nbANsCpyb5Hrgg8AuffMvrKq7qup3wI3AY4CnAd8aqaWqlo9xDwcAH2nqWwA8vAnNlwL/kuSvgUeM\n1ZaSZF6SxUkWz58/f+U/KUmSpCHjSvTqu7uqdu0fSALwm/4h4I1VtXDUvD9dxbnvaf58gD/8Hb0L\nuKiqXpRkDnDxGPNHH7Mqs4CnNeG730lJ/gP4U3qr5wdV1ff7J1TVfGAkPdsTLUmS1iquRE+thcDr\nkqwLkGTHJBvRW+l9cdMbvRWw7wTOtSlwe7N99ATmXw48K8n2zbXHauc4H3jjyJskuzZ/Pq6qrquq\n9wGLgJ0mcD1JkqS1hiF6an2KXnvFVU0bxiforRJ/EVjW7PsccBVw1yrO9X7gvUmuZgIrzVX1S2Ae\n8KUk1wDnjDHtr4E9mg893gi8thn/2+bRfdcC9wH/uarrSZIkrU18xN2AJNm4qn6dZAvgCuAZTX/0\nTOQ/IkmSpicfcTdF7IkenK8leQSwHvCuGRygJUmS1jquRKsL/iOSJGl6ciV6itgTLUmSJLVkiJYk\nSZJaMkRLkiRJLRmiJUmSpJYM0ZIkSVJLhmhJkiSpJUO0JEmS1JIhWpIkSWrJEC1JkiS1ZIiWJEmS\nWjJES5IkSS0ZoiVJkqSWDNGSJElSS4ZoSZIkqSVDtCRJktSSIVqSJElqyRAtSZIktWSIliRJkloy\nREuSJEktGaIlSZKklgzRkiRJUkvrDLoAzXzLzzhn0CV0bvNXvHTQJUiSpGnMlWhJkiSpJUO0JEmS\n1JIhWpIkSWrJEC1JkiS1ZIiWJEmSWjJES5IkSS0ZoiVJkqSWDNGSJElSS4ZoSZIkqSVDtCRJktSS\nIVqSJElqyRAtSZIktWSIliRJkloyREuSJEktGaIlSZKklgzRkiRJUkuGaEmSJKklQ7QkSZLUkiFa\nkiRJaskQLUmSJLVkiJYkSZJaMkRLkiRJLRmip5EkWyU5K8ktSa5MclmSFyXZN8nXBl2fJEmSegzR\n00SSAF8BvlVVj62q3YHDgW0GW5kkSZJGM0RPH/sB91bVx0cGqupHVfXh/klJ3pHkuL731yeZ02wf\nmeTaJNckOaMZm5PkG834hUm2a8Zf0hx7TZJvNWOzk5ycZFEz/zVTfteSJEkz0DqDLkC/twtw1eoe\nnGQX4G3An1TVHUk2b3Z9GDi9qk5P8krgQ8ALgROAg6rq9iSPaOYeA9xVVXsmWR+4NMn5VXXr6tYl\nSZI0jFyJnqaSnNKsEi+a4CH7AedW1R0AVbW8GX86cFazfQawT7N9KXBaklcDs5uxA4EjkywBvgts\nAcwdp755SRYnWXz6Rf/V5tYkSZJmPFeip48bgBePvKmqNyTZElg8at79PPiXnw1W52JV9dokewPP\nB65MsjsQ4I1VtXACx88H5gMsP+OcWp0aJEmSZipXoqePbwAbJHld39jDxph3G7AbQJLdgO37jn9J\nki2afSPtHN+h9wFFgCOAbzf7H1dV362qE4BfAtsCC4HXJVm3mbNjko26uT1JkqTh4Ur0NFFVleSF\nwAeT/D/0gu1vgDePmvpFei0XN9BrufhBc/wNSd4DfDPJA8DVwNHAG4HPJPmH5px/1Zzn5CRz6a0+\nXwhcA1wLzAGuap4W8kt6/dOSJEnqkyr/J16TM4ztHJu/4qWDLkGSpC5k0AUMK9s5JEmSpJYM0ZIk\nSVJLhmhJkiSpJUO0JEmS1JIhWpIkSWrJEC1JkiS1ZIiWJEmSWjJES5IkSS0ZoiVJkqSWDNGSJElS\nS4ZoSZIkqSVDtCRJktSSIVqSJElqyRAtSZIktWSIliRJkloyREuSJEktGaIlSZKklgzRkiRJUkuG\naEmSJKklQ7QkSZLUkiFakiRJailVNegaNPP5j0iSpOkpgy5gWLkSLUmSJLW0zqAL0Mx356c+O+gS\nOrfFq44E4I6PfnrAlXRvy9cfM+gSJEma8VyJliRJkloyREuSJEktGaIlSZKklgzRkiRJUkuGaEmS\nJKklQ7QkSZLUkiFakiRJaskQLUmSJLVkiJYkSdK0l+TgJDclWZrk+DH2vzbJdUmWJLkkyc59+97S\nHHdTkoO6qMcQLUmSpGktyWzgFOB5wM7Ay/pDcuOsqnpSVe0KvB/4l+bYnYHDgV2Ag4GPNuebFEO0\nJEmSpru9gKVVdUtV3QucDRzaP6Gq/rfv7UZANduHAmdX1T1VdSuwtDnfpKwz2RNIkiRJo928z0G1\n6ll/sOOl578GmNc3NL+q5jfbWwM/6du3DNh79DmSvAF4E7AesF/fsZePOnbrNrWNxRAtSZKkgWsC\n8/xVTlz5OU4BTknyl8DbgKO6qG0stnNIkiRpursd2Lbv/TbN2HjOBl64msdOiCFakiRJ3cusdq+V\nWwTMTbJ9kvXofVBwwYMul8zte/t84OZmewFweJL1k2wPzAWumOzt2c4hSZKk7iWdnaqq7k9yLLAQ\nmA2cWlU3JDkRWFxVC4BjkxwA3Af8iqaVo5n3eeBG4H7gDVX1wGRrMkRLkiRp2quq84DzRo2d0Lf9\nNys59j3Ae7qsxxAtSZKkzmVWdyvR05EhWpIkSd1bdZ/zjDbcdydJkiRNAVeiJUmS1L3Zw71WO9x3\ntxqSPJBkSZJrklyV5E+a8TlJru/oGhcn2aPZvi3JdUmuTXJ+kkd1cQ1JkiRNHUP0Q91dVbtW1VOA\ntwDvXQPXfE5VPRlYDPxj/470rJG/pyT+z4QkSepEklavmcYQvXIPp/ecwQdJskGSzzQryFcnec4q\nxjdMcnaS7yX5MrDhONf7FrBDs+p9U5LPAtcD2yb5hySLmhXrdzbn3SjJfzSr5tcneWkzflKSG5u5\nH2jGTktyWN89/Lr5c98k306yALgxyewkJ/dd6zUd/SwlSdLaZNasdq8ZxpXHh9owyRJgA+DRwH5j\nzHkDUFX1pCQ7Aecn2XEl468DfltVT0jyZOCqca79AuC6ZnsucFRVXZ7kwOb9XkCABUmeBTwS+O+q\nej5Akk2TbAG8CNipqirJIyZwz7sBT6yqW5PMA+6qqj2TrA9cmuT8qrp1AueRJEnqmYGry23MvNg/\n9UbaOXYCDgY+m4f+H8M+wOcAqur7wI+AHVcy/qy+8WuBa0ed76ImuD+cP7SP/KiqLm+2D2xeV9ML\n4DvRC9XXAc9N8r4kz6yqu4C7gN8Bn07y58BvJ3DPV/SF5AOBI5t6vgts0VzrQZLMS7I4yeLTv3XR\nBC4hSZI0PFyJXomquizJlvRWfKfSc6rqjpE3zerxb/r2B3hvVX1i9IFJdgP+FHh3kgur6sQkewH7\nA4cBx9JbTb+f5pempsd6vb7TjL7WG6tq4coKrqr5wHyAOz/12ZrojUqSpLWEK9Frr6YlYzZw56hd\n3waOaObsCGwH3LSS8W8Bf9mMPxF4cstSFgKvTLJxc46tk/xRkj+m1ybyOeBkYLdmzqbNV2P+HfCU\n5hy3Abs324cA667kWq9Lsu7IfSTZqGW9kiRpLZdZs1q9ZhpXoh9qpCcaequyR1XVA6M6Oj4KfCzJ\ndfRWeI+uqnuSjDf+MeAzSb4HfA+4sk1BVXV+kicAlzV1/Bp4ObADcHKSFcB99HqvNwG+mmSDpv43\nNaf5ZDN+DfB1Hrz63O9TwBzgqqaN5ZfAC9vUK0mSNOxS5f/Ea3KGsZ1ji1cdCcAdH/30gCvp3pav\nP2bQJUiS1pyB9VTc8meHt8oHj/33s2dU/4cr0ZIkSeqePdGSJEmS+rkSLUmSpM7NxG8hbMMQLUmS\npO7NGu4QbTuHJEmS1JIr0ZIkSepehnut1hAtSZKk7tnOIUmSJLWTpNVrAuc7OMlNSZYmOX6M/W9K\ncmOSa5NcmOQxffseSLKkeS3o4v5ciZYkSdK0lmQ2cArwXGAZsCjJgqq6sW/a1cAeVfXbJK8D3g+8\ntNl3d1Xt2mVNrkRLkiSpe5nV7rVyewFLq+qWqroXOBs4tH9CVV1UVb9t3l4ObNP5PfUxREuSJKl7\ns9LutXJbAz/pe7+sGRvPMcB/9r3fIMniJJcneeHq3dCD2c4hSZKkzmVWu7XaJPOAeX1D86tqfuvr\nJi8H9gCe3Tf8mKq6PcljgW8kua6qftj23P0M0ZIkSepey28sbALzeKH5dmDbvvfbNGOjLpkDgLcC\nz66qe/rOfXvz5y1JLgaeCkwqRNvOIUmSpOluETA3yfZJ1gMOBx70lI0kTwU+ARxSVb/oG98syfrN\n9pbAM4D+DySuFleiJUmS1L2WK9ErU1X3JzkWWAjMBk6tqhuSnAgsrqoFwMnAxsC5zSPzflxVhwBP\nAD6RZAW9BeSTRj3VY7UYoiVJktS9lj3Rq1JV5wHnjRo7oW/7gHGO+w7wpE6LwXYOSZIkqTVXoiVJ\nktS5iXwL4UxmiJYkSVL3Vv3s5xnNEC1JkqTurfpbCGe04b47SZIkaQq4Eq1J2+JVRw66hCmz5euP\nGXQJkiTNTPZES5IkSe3Enmhp5S656bZBl9C5fR4/B4C7l1w32EKmwIa7Ponb//5tgy6jc1v/87sH\nXYIkaS1iiJYkSVL3bOeQJEmSWur4Gwunm+G+O0mSJGkKuBItSZKkzmXIV6IN0ZIkSerekPdED/ev\nCJIkSdIUcCVakiRJ3RvylWhDtCRJkrpnT7QkSZLUTlyJliRJkloyREuSJEktzRruED3czSqSJEnS\nFHAlWpIkSd3LcK/VDvfdSZIkaSAyK61eqzxfcnCSm5IsTXL8GPvflOTGJNcmuTDJY/r2HZXk5uZ1\nVBf3Z4iWJEnStJZkNnAK8DxgZ+BlSXYeNe1qYI+qejLwBeD9zbGbA28H9gb2At6eZLPJ1mSIliRJ\nUvdmzWr3Wrm9gKVVdUtV3QucDRzaP6GqLqqq3zZvLwe2abYPAi6oquVV9SvgAuDgSd/eZE8gSZIk\nPUTS6pVkXpLFfa95fWfbGvhJ3/tlzdh4jgH+czWPnRA/WChJkqSBq6r5wPzJnifJy4E9gGdPuqiV\ncCVakiRJnUtvdXnCr1W4Hdi27/02zdjoax4AvBU4pKruaXNsW4ZoSZIkda/bnuhFwNwk2ydZDzgc\nWNA/IclTgU/QC9C/6Nu1EDgwyWbNBwoPbMYmxXYOSZIkda/Dr/2uqvuTHEsv/M4GTq2qG5KcCCyu\nqgXAycDGwLnNyvaPq+qQqlqe5F30gjjAiVW1fLI1GaLHkOStwF8CDwArgNcA59B7bModo+Z+p6r+\nZCXn+jKwPb2/1EcCtza7Xg+cNc45DwF2rqqTxjnnHOBrVfXE1jcnSZI0A1XVecB5o8ZO6Ns+YCXH\nngqc2mU9huhRkjwdeAGwW1Xdk2RLYL3x5q8sQDf7X9Scd1/guKp6Qd+1xjtmAaP+i0KSJGlG6XAl\nejqyJ/qhHg3cMdKMXlV3VNV/j+xMsmGS/0zy6ub9r5s/901ycZIvJPl+kjMzgS554I1JrkpyXZKd\nmnMdneQjzfZWSb6c5Jrm9aDQnuSxSa5Osmdz3JeSfL35Rp739807MMllzbXOTbJxM35S37f7fKAZ\ne0mS65vrfWsyP0xJkrR2yqxZrV4zzcyreOqdD2yb5AdJPpqk//EoGwP/DvxbVX1yjGOfCvwtvW/S\neSzwjAlc746q2g34GHDcGPs/BHyzqp4C7AbcMLIjyeOBLwJHV9VIn8+uwEuBJwEvTbJts5r+NuCA\n5lqLgTcl2QJ4EbBL8+0+727OcQJwUHPNQyZwD5IkSWsVQ/QoVfVrYHdgHvBL4JwkRze7vwp8pqo+\nO87hV1TVsqpaASwB5kzgkl9q/rxynPn70QvYVNUDVXVXM/7Ipp4jquqavvkXVtVdVfU74EbgMcDT\n6AX7S5MsAY5qxu8Cfgd8OsmfAyPf8nMpcFqz2j57rKL7H4i+4JyzJnCbkiRprdLyy1ZmGnuix1BV\nDwAXAxcnuY5e6IReuDw4yVlVVWMcek/f9gNM7Oc7csxE54+4C/gxsA+9sLyyGkLv6y5fNvokSfYC\n9gcOA44F9quq1ybZG3g+cGWS3avqzv7j+h+IfslNt431s5AkSWuzWTMvGLfhSvQoSR6fZG7f0K7A\nj5rtE4BfAaeswZIuBF7X1DY7yabN+L30WjGOTPKXqzjH5cAzkuzQnGejJDs2fdGbNp92/TvgKc3+\nx1XVd5tPvP6SBz+gXJIkaa1niH6ojYHTRz5sR68N4h19+/8G2LD/Q3tT7G+A5zQr4lc29QBQVb+h\n9ySRv2seizemqvolcDTwb809XQbsBGwCfK0ZuwR4U3PIyc0HHa8HvgNc89CzSpIkrcSQt3Nk7K4E\naeKGsZ1jn8fPAeDuJdcNtpApsOGuT+L2v3/boMvo3Nb//O5VT5Kktc/A0unP3/OBVvlgq7ceN6OS\ntD3RkiRJ6l6Gu+HBEC1JkqTuDfkHCw3RkiRJ6tzEvnNu5hrudXZJkiRpCrgSLUmSpO7ZEy1JkiS1\nNOQ90cP9K4IkSZI0BVyJliRJUveG/IOFhmhJkiR1LrZzSJIkSernSrQkSZK659M5JEmSpJaGvCd6\nuH9FkCRJ0lBIcnCSm5IsTXL8GPufleSqJPcnOWzUvgeSLGleC7qox5VoSZIkda/DDxYmmQ2cAjwX\nWAYsSrKgqm7sm/Zj4GjguDFOcXdV7dpZQRiiJUmSNAUyq9OGh72ApVV1C0CSs4FDgd+H6Kq6rdm3\nossLj8d2DkmSJHUvs9q9Vm5r4Cd975c1YxO1QZLFSS5P8sK2tzIWV6IlSZI0cEnmAfP6huZX1fyO\nTv+Yqro9yWOBbyS5rqp+OJkTGqIlSZLUvZY90U1gHi803w5s2/d+m2Zsoue+vfnzliQXA08FJhWi\nbeeQJElS55K0eq3CImBuku2TrAccDkzoKRtJNkuyfrO9JfAM+nqpV5chWpIkSdNaVd0PHAssBL4H\nfL6qbkhyYpJDAJLsmWQZ8BLgE0luaA5/ArA4yTXARcBJo57qsVps55AkSVL3Ov6ylao6Dzhv1NgJ\nfduL6LV5jD7uO8CTOi0GQ7QkSZKmQrePuJt2UlWDrkEzn/+IJEmangb23dt3fvqMVvlgi2NeMaO+\nJ9yVaE3ad3/4k1VPmmH2flzvA8A373PQgCvp3txLFvJ/F35z0GV0bpP9nw0M39/Z3EsWDroESVot\nE/iw4Iw23OvskiRJ0hRwJVqSJEndG/KeaEO0JEmSujfk7RyGaEmSJHVvyFeih/vuJEmSpCngSrQk\nSZI6l1m2c0iSJEntDHlPtO0ckiRJUkuuREuSJKl7Ge61WkO0JEmSOjfsPdHD/SuCJEmSNAVciZYk\nSVL3hvyDhYZoSZIkdW/Ie6KH++4kSZKkKeBKtCRJkro35B8sNERLkiSpc7EnWpIkSWppyFei7YmW\nJEmSWjJES5IkqXuzZrV7rUKSg5PclGRpkuPH2P+sJFcluT/JYaP2HZXk5uZ1VBe3ZzuHJEmSutfh\nI+6SzAZOAZ4LLAMWJVlQVTf2TfsxcDRw3KhjNwfeDuwBFHBlc+yvJlOTK9GSJEnqXJJWr1XYC1ha\nVbdU1b3A2cCh/ROq6raquhZYMerYg4ALqmp5E5wvAA6e7P25Ei1JkqTudfvBwq2Bn/S9XwbsPYlj\nt55sQa5EzxBJft3x+eYkub7Z3iPJh7o8vyRJUhtJ5iVZ3PeaN+iaVsaVaFFVi4HFg65DkiQNkZbP\nia6q+cD8cXbfDmzb936bZmwibgf2HXXsxa2KG4Mr0TNMkn2TXJzkC0m+n+TMNI1ESU5KcmOSa5N8\noBk7rf8TqmOtaDfn/Fqz/Y4kpzbXuCXJX6+pe5MkSUMks9q9Vm4RMDfJ9knWAw4HFkywkoXAgUk2\nS7IZcGAzNimuRM9MTwV2Af4buBR4RpLvAS8CdqqqSvKISZx/J+A5wCbATUk+VlX3TbZoSZKk1VFV\n9yc5ll74nQ2cWlU3JDkRWFxVC5LsCXwZ2Az4syTvrKpdqmp5knfRC+IAJ1bV8snW5Er0zHRFVS2r\nqhXAEmAOcBfwO+DTSf4c+O0kzv8fVXVPVd0B/ALYavSE/r6lr5x95iQuJUmShlFmpdVrVarqvKra\nsaoeV1XvacZOqKoFzfaiqtqmqjaqqi2qape+Y0+tqh2a12e6uD9Xomeme/q2HwDWaX5D2wvYHzgM\nOBbYD7if5pelJLOA9Vbn/KMn9PctffeHP6nVuAdJkjTMWvZEzzSG6CGRZGPgYVV1XpJLgVuaXbcB\nuwOfBw4B1h1MhZIkaa0ygW8hnMkM0cNjE+CrSTYAArypGf9kM34N8HXgNwOqT5IkaWgYomeIqtq4\n+fNi+h7LUlXH9k3ba4zjfg48rW/ozc34bcATR5+zqt4x6vgnTrZ2SZK09pnAtxDOaIZoSZIkdW/I\n2zmG++4kSZKkKeBKtCRJkrpnO4ckSZLU0gSe/TyT2c4hSZIkteRKtCRJkjrX+4634WWIliRJUveG\nvCd6uH9FkCRJkqaAK9GSJEnq3pB/sNAQLUmSpO7ZEy1JkiS1E1eiJUmSpJaG/IOFhmhJkiR1b8hD\n9HA3q0iSJElTwJVoSZIkdS6zhnut1hAtSZKk7g15iB7uu5MkSdJQSHJwkpuSLE1y/Bj7109yTrP/\nu0nmNONzktydZEnz+ngX9bgSLUmSpO51+MHCJLOBU4DnAsuARUkWVNWNfdOOAX5VVTskORx4H/DS\nZt8Pq2rXzgrClWhJkiRNhVlp91q5vYClVXVLVd0LnA0cOmrOocDpzfYXgP2TqXtEiCFakiRJ093W\nwE/63i9rxsacU1X3A3cBWzT7tk9ydZJvJnlmFwXZziFJkqTOpeXXfieZB8zrG5pfVfM7KOWnwHZV\ndWeS3YGvJNmlqv53Mic1RGvS9n7ctoMuYcrMvWThoEuYEpvs/+xBlzBlhvXvTJJmnJadFE1gHi80\n3w70B45tmrGx5ixLsg6wKXBnVRVwT3ONK5P8ENgRWNyqwFFs55AkSVL3uu2JXgTMTbJ9kvWAw4EF\no+YsAI5qtg8DvlFVleSRzQcTSfJYYC5wy2Rvz5VoTdp9P/v5oEvo3LqP2gqAZW84bsCVdG+bUz7A\nD5/7wkGX0bnHXfAVAG7e56ABV9KtkZX1n77lnQOupHuPfu/bB12CpBmiqu5PciywEJgNnFpVNyQ5\nEVhcVQuATwNnJFkKLKcXtAGeBZyY5D5gBfDaqlo+2ZoM0ZIkSepexw/GqKrzgPNGjZ3Qt/074CVj\nHPdF4IudFoMhWpIkSVOg7QcLZ5rhvjtJkiRpCrgSLUmSpO6t+sOCM5ohWpIkSd2bNdwND8N9d5Ik\nSdIUcCVakiRJnUvHT+eYbgzRkiRJ6t6Qt3MYoiVJktQ9V6IlSZKklgzRkiRJUjsZ8kfcDXeziiRJ\nkjQFXImWJElS94b8a78N0ZIkSerekPdED/evCJIkSdIUcCVakiRJ3RvyDxYaoiVJktS5DHlP9HDf\nnSRJkjQFXImWJElS92znkCRJktq5e4P1W83fZIrqmCq2c0iSJEktGaKHXJIHkixJck2Sq5L8STM+\nJ0kleXff3C2T3JfkI837dyQ5blC1S5IkTVeG6OF3d1XtWlVPAd4CvLdv363A8/vevwS4YU0WJ0mS\nNBFJDk5yU5KlSY4fY//6Sc5p9n83yZy+fW9pxm9KclAX9Rii1y4PB37V9/63wPeS7NG8fynw+TVe\nlSRJ0kokmQ2cAjwP2Bl4WZKdR007BvhVVe0AfBB4X3PszsDhwC7AwcBHm/NNiiF6+G3YtHN8H/gU\n8K5R+88GDk+yLfAA8N9rukBJkqRV2AtYWlW3VNW99PLLoaPmHAqc3mx/Adg/SZrxs6vqnqq6FVja\nnG9SDNHDb6SdYyd6v319tvkHNeLrwHPp/YZ2zkRPmmReksVJFn/qjDO6rViSJOnBtgZ+0vd+WTM2\n5pyquh+4C9higse25iPu1iJVdVmSLYFH9o3dm+RK4O/p/ffIIRM813xgPsB9P/t5TUG5kiRpLZJk\nHjCvb2h+kzemJUP0WiTJTsBs4E7gYX27/hn4ZlUtf/AitSRJ0prRv0A3htuBbfveb9OMjTVnWZJ1\ngE3pZZ6JHNua7RzDb6Qnel/imsoAACAASURBVAm9do2jquqB/glVdUNVnT724ZIkSQO3CJibZPsk\n69FrQ10was4C4Khm+zDgG1VVzfjhzdM7tgfmAldMtiBXoodcVY356dOqug144hjjpwGnNdvvmLrK\nJEmSJqaq7k9yLLCQ3v+qn1pVNyQ5EVhcVQuATwNnJFkKLKcXtGnmfR64EbgfeMPoBcXVYYiWJEnS\ntFdV5wHnjRo7oW/7d/S+82KsY98DvKfLemznkCRJkloyREuSJEkt2c4hSZKkzt03e91BlzClDNGS\nJEnqXA35t0gYoiVJktS5FUOeou2JliRJklpyJVqSJEmdqyFfiTZES5IkqXPDHqJt55AkSZJaciVa\nkiRJnRv2DxYaoiVJktS5Ic/QtnNIkiRJbbkSLUmSpM4N+wcLDdGSJEnq3AqGO0TbziFJkiS15Eq0\nJEmSOmc7hyRJktSSj7iTJEmSWlqxYrhDtD3RkiRJUkuuREuSJKlzw74SnWFv+tYa4T8iSZKmpwzq\nwkt/vrxVPthhq81Xu9YkmwPnAHOA24C/qKpfjTHvKOBtzdt3V9XpzfjFwKOBu5t9B1bVL1Z2Tds5\nJEmS1LkVVKvXJB0PXFhVc4ELm/cP0gTttwN7A3sBb0+yWd+UI6pq1+a10gANtnOoAz9eftegS+jc\ndptvCsDyUz834Eq6t/krX849Ny0ddBmdW//xOwDwsxPfN+BKuvWoE94MDN99Qe/e7vzUZwddRue2\neNWRgy5BmhbWcLfDocC+zfbpwMXAm0fNOQi4oKqWAyS5ADgY+LfVuaAr0ZIkSZrptqqqnzbbPwO2\nGmPO1sBP+t4va8ZGfCbJkiT/b5JVtpa4Ei1JkqTOtV2JTjIPmNc3NL+q5vft/y/gUWMc+tZR160k\nbZfBj6iq25NsAnwReAWw0v8qM0RLkiSpc20fztEE5vkr2X/AePuS/DzJo6vqp0keDYzV03w7f2j5\nANiGXtsHVXV78+f/JTmLXs/0SkO07RySJEma6RYARzXbRwFfHWPOQuDAJJs1Hyg8EFiYZJ0kWwIk\nWRd4AXD9qi7oSrQkSZI6t4Y/WHgS8PkkxwA/Av4CIMkewGur6lVVtTzJu4BFzTEnNmMb0QvT6wKz\ngf8CPrmqCxqiJUmS1Lk1GaKr6k5g/zHGFwOv6nt/KnDqqDm/AXZve01DtCRJkjq3Ysi/0M+eaEmS\nJKklV6IlSZLUuWFfiTZES5IkqXNr+IOFa5ztHJIkSVJLrkRLkiSpc7ZzSJIkSS0NeYa2nUOSJElq\ny5VoSZIkdW7YP1hoiJYkSVLn7ImWJEmSWhr2lWh7oiVJkqSWXImWJElS5x5Y4Uq0JEmSpD6uREuS\nJKlzw94TbYiWJElS54b96RyrbOdI8kCSJUluSHJNkr9PMqvZt0eSD63i+KOTfKRNUUn+sc38Ucee\nluTWpuarkjy9xbG/rzXJa5Mcubp1TPB6c5Lc3dQ68lqvw/MfneSP+95/KsnOXZ1fkiRpbTWRlei7\nq2pXgCR/BJwFPBx4e1UtBhZPQV3/CPzTJI7/h6r6QpIDgU8AT257gqr6eJv5SdapqvvbXgf44cjP\ndwocDVwP/DdAVb1qiq4jSZL0IGv9SnS/qvoFMA84Nj37JvkaQJK9klyW5Ook30ny+L5Dt01ycZKb\nk7x9ZDDJy5Nc0azAfiLJ7CQnARs2Y2euZN7sZtX5+iTXJfm7MUr+FrDDeOdoxv8qyQ+SXAE8o6+2\ndyQ5rtneM8m1zbEnJ7m+GT86yYIk3wAuTLJRklOb61yd5NBm3uzmuEXNeV6zsp9zkl/3bR+W5LRm\n+7QkH2p+vrckOaxv3pubn8M1SU5q9u0BnNnUvWHzd7BHM/9lzfzrk7yv/9pJ3tOc5/IkW62sVkmS\npLFUVavXTNP66RxVdQswG/ijUbu+Dzyzqp4KnMCDV5L3Al5Mb0X4JU0byBOAlwLPaFZiHwCOqKrj\naVa/q+qI8eYBuwJbV9UTq+pJwGfGKPfPgOvGO0eSRwPvpBee9wHGa3X4DPCavmP77QYcVlXPBt4K\nfKOq9gKeA5ycZCPgGOCuqtoT2BN4dZLtm+Mf19fKcco41+/36KbWFwAnASR5HnAosHdVPQV4f1V9\ngd7/EhzR/CzvHjlB0+LxPmA/ej/HPZO8sNm9EXB5c55vAa8eq4gk85IsTrL4rNNPm0DZkiRJw6PL\nDxZuCpyeZC5QwLp9+y6oqjsBknyJXgi8H9gdWJQEYEPgF2Ocd/9x5v078NgkHwb+Azi/75iTk7wN\n+CW9ADveOfYGLq6qXza1nQPs2H/xJI8ANqmqy5qhs+gF2P57W95sHwgcMrKCDWwAbNeMP7lv5XhT\nYC7wA9q3c3ylqlYAN/atEh8AfKaqfgvQV8949uTB930m8CzgK8C9wNeaeVcCzx3rBFU1H5gP8OPl\nd828Xx8lSdKUmomry220DtFJHktvNfYXwBP6dr0LuKiqXpRkDnBx377RP8UCApxeVW9Z1SXHm5fk\nKcBBwGuBvwBe2ez6h2YldmTec8Y6R9/q62T8ZlStL66qm0ZdJ8Abq2rhqPE545yz/+e1wah994y6\nXtfuqz/8q38An+AiSZJWw5r8rpUkmwPnAHOA24C/qKpfjTHv68DTgEuq6gV949sDZwNb0FtEfEVV\n3buya7Zq50jySODjwEfqob9ebArc3mwfPWrfc5NsnmRD4IXApcCFwGHpfViRZv9jmvn3JRlZyR5z\nXpItgVlV9UXgbfTaKsYz3rW+Czw7yRbN9V4y+sCq+h/g/5Ls3QwdvpLrLATe2IRmkjy1b/x1I/eU\nZMemzWM8P0/yhPSegvKilcwbcQHwV0keNnJ/zfj/AZuMMf8Keve9ZdMb/jLgmxO4jiRJ0oSs4Z7o\n44ELq2ouvdx3/DjzTgZeMcb4+4APVtUOwK/odTKs1ERC9MiH/G4A/ote28Q7x5j3fuC9Sa7moauX\nVwBfBK4FvlhVi6vqRnrh9/wk19ILgo9u5s8Hrk1y5krmbQ1cnGQJ8Dlg3BXt8c5RVT8F3gFcRi/Y\nf2+cUxwDfLK51kbAXePMexe9NpZrm5/Xu5rxTwE3Alc1H0r8xBg/o37H02up+A7w05XMG7m/rwML\ngMVNjSPtJKcBHx/5YGHf/J8217gIuAa4sqq+uqrrSJIkTVOHAqc326fTW7R9iKq6kN4i4+81i5/7\nASNdDOMe/6Djhr1fpQtJNq6qXzfbx9ML4H8z4LKmjWHsid5u800BWH7q5wZcSfc2f+XLueempYMu\no3PrP34HAH524vtWMXNmedQJbwaG776gd293fuqzgy6jc1u8akq/YkBqaypaPyfk/Gt/0CofHPjk\nHVe71iT/U1WPaLYD/Grk/Rhz9wWOG2nnaLobLm9WoUmyLfCfVfXElV3TfteJeX6St9D7ef2Ih7ar\nSJIkqc+Kh3wkbuWSzKP3KOUR85sHGYzs/y/gUWMc+tb+N1VVSaZ8gc8QPQFVdQ69ZnVJkiRNgf4n\nf42z/4Dx9iX5eZJHV9VPm0cYj/XEt/HcCTwif/jivG34w+f8xtX6OdGSJEnSqqzhDxYuAI5qto8C\nJvxZr+ZhGRcBI48intDxhmhJkiR1bkW1e03SSfSeBnczve/PGPlCuj2SfGpkUpJvA+cC+ydZluSg\nZtebgTclWUrvMXefXtUFbeeQJEnSjNZ8qd/+Y4wvBl7V9/6Z4xx/C71v2J4wQ7QkSZI6t2JNftvK\nABiiJUmS1Llhf4yyPdGSJElSS65ES5IkqXOH7L7zwL7oZU1wJVqSJElqyRAtSZIktWSIliRJkloy\nREuSJEktGaIlSZKklgzRkiRJUkuGaEmSJKklQ7QkSZLUkiFakiRJaskQLUmSJLVkiJYkSZJaMkRL\nkiRJLaWqBl2DZj7/EUmSND1l0AUMq3UGXYBmvpv3OWjQJXRu7iULAbj1RUcMuJLubf/lM/nxUa8b\ndBmd2+70jwHw1cU3DLiSbh26xy4ALP358gFX0r0dttqce35466DL6Nz6j9segHtvuW2whXRsvcfO\nGXQJ0rRiO4ckSZLUkiFakiRJaskQLUmSJLVkiJYkSZJaMkRLkiRJLRmiJUmSpJYM0ZIkSVJLhmhJ\nkiSpJUO0JEmS1JIhWpIkSWrJEC1JkiS1ZIiWJEmSWjJES5IkSS0ZoiVJkqSWDNGSJElSS4ZoSZIk\nqSVDtCRJktSSIVqSJElqyRAtSZIktWSIliRJkloyREuSJEktGaIlSZKklgzRQy7JC5NUkp0GXYsk\nSdKwMEQPv5cBlzR/SpIkqQOG6CGWZGNgH+AY4PBmbFaSjyb5fpILkpyX5LBm3+5JvpnkyiQLkzx6\ngOVLkiRNW4bo4XYo8PWq+gFwZ5LdgT8H5gA7A68Ang6QZF3gw8BhVbU7cCrwnkEULUmSNN2tM+gC\nNKVeBvxrs312834d4NyqWgH8LMlFzf7HA08ELkgCMBv46XgnTjIPmAdw4uN25vBHbTMlNyBJkjQd\nGaKHVJLNgf2AJyUpeqG4gC+PdwhwQ1U9fSLnr6r5wHyAm/c5qCZfsSRJ0sxhO8fwOgw4o6oeU1Vz\nqmpb4FZgOfDipjd6K2DfZv5NwCOT/L69I8kugyhckiRpujNED6+X8dBV5y8CjwKWATcCnwOuAu6q\nqnvpBe/3JbkGWAL8yZorV5IkaeawnWNIVdVzxhj7EPSe2lFVv06yBXAFcF2zfwnwrDVaqCRJ0gxk\niF47fS3JI4D1gHdV1c8GXZAkSdJMYoheC1XVvoOuQZIkaSazJ1qSJElqyRAtSZIktWSIliRJkloy\nREuSJEktGaIlSZKklgzRkiRJUkuGaEmSJKklQ7QkSZLUkiFakiRJaskQLUmSJLVkiJYkSZJaMkRL\nkiRJLRmiJUmSpJYM0ZIkSVJLhmhJkiSpJUO0JEmS1JIhWpIkSWrJEC1JkiS1ZIiWJEmSWkpVDboG\nzXz+I5IkaXrKoAsYVq5ES5IkSS2tM+gCNPMte8Nxgy6hc9uc8gEAbt7noAFX0r25lyzkt4uuGnQZ\nnXvYnrsB8JtLvzvgSrq10TP2BmD5GecMuJLubf6Kl3Lzs/500GV0bu63zgPgF+/7/wZcSbf+6M1/\nC8CP/+r1A66ke9t95qODLkEzkCvRkiRJUkuGaEmSJKklQ7QkSZLUkiFakiRJaskQLUmSJLVkiJYk\nSZJaMkRLkiRJLRmiJUmSpJYM0ZIkSVJLhmhJkiSpJUO0JEmS1JIhWpIkSWrJEC1JkiS1ZIiWJEmS\nWjJES5IkSS0ZoiVJkqSWDNGSJElSS4ZoSZIkqSVDtCRJktSSIVqSJElqyRAtSZIktWSIHpAk2yT5\napKbk/wwyb8mWW8Vx/zjmqpPkiRJ4zNED0CSAF8CvlJVc4EdgY2B96ziUEO0JEnSNGCIHoz9gN9V\n1WcAquoB4O+AVyZ5fZKPjExM8rUk+yY5CdgwyZIkZzb7jkxybZJrkpzRjM1J8o1m/MIk2zXjpyX5\nWJLLk9zSnPPUJN9Lclrf9Q5MclmSq5Kcm2TjNfZTkSRJmiEM0YOxC3Bl/0BV/S/wY2CdsQ6oquOB\nu6tq16o6IskuwNuA/arqKcDfNFM/DJxeVU8GzgQ+1HeazYCn0wvsC4APNrU8KcmuSbZsznlAVe0G\nLAbe1MUNS5IkDRND9My1H3BuVd0BUFXLm/GnA2c122cA+/Qd8+9VVcB1wM+r6rqqWgHcAMwBngbs\nDFyaZAlwFPCYsS6eZF6SxUkWn3nDtd3emSRJ0jQ35qqnptyNwGH9A0keDmwH/A8P/uVmgw6ve0/z\n54q+7ZH36wAPABdU1ctWdaKqmg/MB1j2huOqwxolSZKmPVeiB+NC4GFJjgRIMhv4Z+A04BZg1ySz\nkmwL7NV33H1J1m22vwG8JMkWzTk2b8a/AxzebB8BfLtFXZcDz0iyQ3POjZLs2PbmJEmShp0hegCa\nlooX0QvBNwM/AH5H7+kblwK30lut/hBwVd+h84Frk5xZVTfQe5rHN5NcA/xLM+eNwF8luRZ4BX/o\nlZ5IXb8Ejgb+rTn+MmCn1b1PSZKkYWU7x4BU1U+APxtn9xHjHPNm4M19708HTh8150f0+qVHH3t0\n3/ZtwBPH2fcNYM9V34EkSdLay5VoSZIkqSVDtCRJktSSIVqSJElqyRAtSZIktWSIliRJkloyREuS\nJEktGaIlSZKklgzRkiRJUkuGaEmSJKklQ7QkSZLUkiFakiRJaskQLUmSJLVkiJYkSZJaMkRLkiRJ\nLRmiJUmSpJYM0ZIkSVJLhmhJkiSpJUO0JEmS1JIhWpIkSWrJEC1JkiS1ZIiWJEmSWjJES5IkSS2l\nqgZdgzRhSeZV1fxB19G1Yb0vGN57875mnmG9N+9r5hnme1ubuBKtmWbeoAuYIsN6XzC89+Z9zTzD\nem/e18wzzPe21jBES5IkSS0ZoiVJkqSWDNGaaYa1h2xY7wuG9968r5lnWO/N+5p5hvne1hp+sFCS\nJElqyZVoSZIkqSVDtCRJktSSIVqSJElqyRCtaS/JRklmNds7JjkkybqDrqsLSR6T5IBme8Mkmwy6\npq4M870BJNksyZMHXUdXksxO8sdJtht5DbomSZrO/GChpr0kVwLPBDYDLgUWAfdW1REDLWySkrya\n3gP3N6+qxyWZC3y8qvYfcGmTNqz3luRi4BBgHeBK4BfApVX1pkHWNVlJ3gi8Hfg5sKIZrqqasb8k\nJFnp30lV/cuaqmUqJHkk8GpgDr1/jwBU1SsHVdNkJdkK+Cfgj6vqeUl2Bp5e/397dx5lV1Wnffz7\nJAwJcxBQVFBABILMItjyooDQzgNCK4qooNigAtLaCPoqDShIa2vDq3QrSjO0E4oy2DKIMkkDEqYQ\nEOnlPCAySGjmwPP+cc4lN2URSOqm9t3nPp+1alWdfZO1nlqp3Npnn9/+bfsrhaNNmKTlgH8A1rb9\nnvY9cQPb5xSOFhOQleiogWzfD+wKfNH27sDGhTMNwvuAlwBzAWzfCqxRNNHgdPV7W9n2XJqfxVNs\nbwO8vHCmQTiQ5hf6xrY3aT+qnUC3VnySj9qdCawM/BD4ft9Hzf4DOA94Znv9c+CgYmkG6yTgIeDF\n7fXvgaPKxYlBWOrJ/0hEcZL0YuBtwD7t2NSCeQblIdsPSwJA0lJAVx4NdfV7W0rSmsDfAR8tHWaA\nfgvcUzrEINn+p9IZlrDlbB9SOsSArWb7W5IOBbA9T9KjpUMNyHq23yxpDwDb96v3BhnVyiQ6anAQ\ncCjwXdtzJK0L/LhwpkG4WNJhwHRJOwP7A2cXzjQoXf3ejqBZKbvM9k/bn8VbC2cahF8AF0n6Ps1q\nGVB3yYOk4xb2uu0DJivLEnKOpFfZ/q/SQQboPklPo73hlrQt3bm5e1jSdOZ/b+vR938t6pSa6KiG\npOXaso5OaDdL7gPsAohmcnaiO/Cfsl1heTcd/N66SNInxhuveTVX0sPAjcC3gD/Q/Bw+zvbJJXIN\niqR7geWBh4FH2mHbXqlcqomRtCVwPPACmn+71YHdbN9QNNgAtIsJHwNmAufTlLu90/ZFJXPFxGQS\nHUOvLeX4CrCC7bUlbQa81/b+haMNjKRVgWd35JfFVGCO7Q1LZxk0ScfS1DE+AJwLbAp80PZpRYPF\nX2lXNHcH3gzMA74JfNv2X4oGi4VqS782oLnpucX2I0/yV6rR/kxuS/O9XWH7jsKRYoIyiY6hJ+lK\nYDfgLNtbtGM32n5B2WQT8wSdHi63/cGSuQZB0pnAB2z/pnSWQZJ0ne3NJb0ReA1wMHCJ7c0KR1ss\nkj5v+yBJZzNOzbrt1xWINXCSng28hebf6xDbpxaONBCSXgds315eVHunB0m7jjN8DzDb9u2TnWfQ\n2paYz2XBbipnFAsUE5aa6KiC7d+O2YPRhc0mK9ueK+ndNJ0ePiGp+pXo1gxgjqSrgPt6gx2YlPXe\nM18NnG77nsr3BvUmk58pmmIJaksE9gB2Bn5Ac8NaPUnHAFsD/9kOHSjpJbYPLRhrovah6V7R2/Py\nMpp/r3UkHVHzzY+kr9I8uZpDXxtJIJPoimUSHTX4raS/AdwesnIgcHPhTIPQ1U4PAP+3dIAl5BxJ\nP6Mp59iv7dX7YOFMi832rPbzxaWzDJqkI2hudm4GvgEcante2VQD9Spgc9uPAUg6GbiWZhN2rZYC\nNrL9J3i8b/QpwDbAJcy/6avRtrZnlg4Rg5VJdNTg74F/BZ5F01vzfJo+xLXrdXr4Scc6PXRyUgZg\n+yNtXfQ9th+VdD/w+tK5Fpek2Syk9WDlvaI/BvwS2Kz9+FT71EBUfpBMn1WAu9qvVy4ZZEDW6k2g\nW7e3Y3dJqr02+r8lzbR9U+kgMTipiY6IgWs7B/TeXJYBlgbuq7lzADx+6tjBNKeO7Vv7qWOSnrOw\n123/erKyDFqXvzeAtt/wMTSlD6Kpjf6I7W8WDTYBkr4IrA2c3g69Cfgd8GHgHNs7lMo2UZJeCpwF\n3EbT2q5LN3MjK5PoGFqS/tH2sZKOZ/xNT1X3eW03Ox1P0+oI4FLgQNu/K5dq8Np2d6+neZz5kdJ5\nJkLSN2lqNPey/YJ2Un257c0LR4unQNJqwJ1dabXYloNt3V5eZfu2knkmqn2v2BXYrh26G3i67eqf\nPEr6H5ob8NnMr4mu/mZu1OXY7xhmvbrnq2kmLmM/ancSzcrEM9uPs9uxTnHje8Dfls4yAOvZPpa2\nL2/bt7zqnYXQPDmQNLf9eFDSo5Lmls41EZK2lXSRpDMkbSHpRprew3+S9IrS+RaXpA3bz1sCa9Ks\n1P4OeGY7Vq325uYXNC0J3wjsQDf2vwD82fZZtn9p+9e9j9KhYmJSEx1Dy/bZ7eeqD0VYiNVt90+a\n/0PSQcXSDNCYVlVTgBdS8Qa8Pp08dcz2ir2v+58clEs0EP8POIymVvhHwCttX9FOQr9O0+e7RgcD\n+wKfHec1AztObpyJk/R8mg4qewB30PT0Vs3lG+O4VtLXaBZL+k8FTXeOiqWcI4aepAuA3XuHJEia\nAXzDdtUrm5IupFl5/no7tAfwLts7lUs1GJL6bw7mAb8Cvlx7r9dROnVM0rW9vuw16vX0br++2fZG\nfa9V/b0BSJpm+8EnG6uBpMdoytn2sf0/7dgvbK9bNtngjHlP7LHtvSc9TAxMVqKjBqv3nzJm+25J\na5QMNCB709REf45mBely4F1FEw2I7U58H2PZvkDSNcw/dezALpw61tEnB4/1ff3AmNe6sHp0OTC2\nfGO8sRrsSnMYzo8lnUvTkrD6Mql+XX1PHHWZREcNHpW0du/0u3bXffW/BNt6uNoPHxlXx4/Hnkaz\n4WkpYKYkbF9SONNEvbbv696Tg2pb97U2a+u6BUzvq/EWzb9hlSQ9g6bd53RJWzB/srkSsFyxYBPQ\n7pn4nqTlaX7uDgLWkHQC8F3b5xcNOACjspF81KScI4ZeuwnoS8DFNL8w/g+wr+3zigaboPZwhAPH\nlKl8tguP97p2PHaPpE8Db2bMqWMdOIkxKiHpHcA7aZ4W/JT5k+i5wMldqbFt3w93B97ckRK3C4Cv\nMf/AmD2Bt9neuVyqmKhMoqMKbWuq3kanKzryCP2v6jK7UKsJIOnGtgXcicC3bZ8r6foOTKJvATa1\nXf1mwn4df3LQSZLeZPs7pXPEU9Nfo7+wsahLWtxFLZalOZlrLs0j9O0L5xmEKe1qCwCSVqU7JVa9\n47G3Ai6s/XjsPr+gOTima3axPZfmqcGvgOfRHHARw2srSav0LiTNkHRUyUCxUHdK2lPS1PZjT+DO\n0qFiYrISHUOvq4/QJe1F04LrdJpHsrsBn7R96kL/YiXam4Le8djLASt14DCI79AcIX0hC7apqv3g\nn04+OeiyJ3iSdY3tGjcWdl67l+d44MXM30h+QG+vT9SpK6te0W1voDlauVOP0G2fIulq5vd13dX2\nTSUzDYqk3YFz2wn0x2g6BhxFc+Rtzc5qP7qm9+TgAWC/Dj056LKpkpbtvS+2/cuXLZwpnkCXN5KP\nsqxEx9CT9AOaPtH/WzrLIElae7zxLqxMSLrB9qaStqOZPP8z8HHb2xSONiGStrI9a8zYa2yfUyrT\noHTxyUGXSTqEpqtKr//wu4Cz2hM1Y8h0eSP5KMtKdNTgfuC69nCSzjxCB77P/FZ904F1gFuAjYsl\nGpxH28+vBr5k+/sdqdf8sqS9bN8IIGkPmnZc1U+igQ2B50rq/71wSqkwsXC2Py3pBqDXueLI2jsW\nddym45x3UP0m8lGXSXTUoJOP0G1v0n8taUtg/0JxBu33kv4d2Bn4tKRl6cZG5t2Ab0t6K02rxb2A\nXcpGmjhJpwLrAdcx/wbIZBI91Gz/APhB6RzxlEyRNMP23dC5jeQjK+UcEUNE0uyxk+sateUArwBm\n275V0prAJh05NOH5wPeA3wBvtD32NLzqSLoZmOn8QqhGe8rkp4E1aDYmi2bD9UpFg8W4xmwkh6YH\n9qds50a1YrkLiqEnaX3gaGAmfSeN2V63WKgBkHRw3+UUms13fygUZ6Bs3y/pdmA74FaaU/BuLZtq\n8UmazYKnZK4KTAWubE8s3LRMsoG5EXgG8MfSQeIpOxZ4re2bSweJJ9fljeSjLJPoqMFJwCeAzwE7\n0Gyg6UJpwIp9X8+jqZHuxOEJkj5Bc6LaBjT/fksDpzH/yNvavKZ0gCVsNeAmSVex4L6DdBMYXn/K\nBLoekk61/XbgpnHGolIp54ihJ2mW7a36Sx16Y6WzxfgkXQdsAVzT62Xb69hRNtnESNoWmGP73vZ6\nJWAj21eWTTYxkl463rjtiyc7Szw1kv6V5unB91jwxqcTx353zdge3pKm0pS7zSwYKyYoK9FRg4ck\nTQFulfR+4PfACoUzLTZJZ7NgacACOrL697BtSzKApOVLBxqQE2jKbnr+d5yx6mSyXKWVaDoX9W9s\nNZBJ9BCRdChNLfR0SXNpatcBHga+VCxYDERWomPoSdoauBlYBTgSWBk41vYVRYMtpidY9ev9R1QX\nJjSSPgSsT9Od42hgWIs13AAADoNJREFUb+Brto8vGmyCJF1ne/MxY9WusEu6l/Fv6LJJLWKAJB1t\n+9DSOWKwMomOmGSSXg882/YX2uurgNVpJjOH2D59YX9/2EkS8GyavsO70EzIzrN9QdFgAyDpDOAi\nmtVnaFoS7mD7DcVCxUiSdBLj3ADl8I7hJGn78cZtXzLZWWJwMomOode2FPsw8Bz6SpBs7/iEf2mI\nSfoJ8Bbbv22vr6M5MGF54CTbOy3s79egK636xpK0BnAczQ57AxcCB9m+vWiwGDmS3tR3OQ14I/CH\nDhxC1UltGV/PNOBFwKxaf49FIzXRUYPTgX8Dvsz8gyBqtkxvAt26zPadwJ0dqh2+RtLWtn9aOsgg\ntZPlt5TOEWF7gU4+kr4OXFYoTjwJ26/tv5a0FvD5QnFiQDKJjhrMs33Ck/+xaszov7D9/r7L1Sc5\ny5KyDbCnpF8B9zG/xrbW2uF/tH2spOMZ/xF6Vv+itPVpDl6JOvwO2Kh0iJiYTKKjBmdL2h/4Lgu2\ncrqrXKQJuVLSe2x/uX9Q0nuBqwplGrS/LR1gwHr9eK8umiKiNc6m0NuAQwrFiScx5gZ8Cm0L0HKJ\nYhBSEx1DT9Ivxxl2rScWtnW1vd6uvTfRrYBlgTfY/lOpbBPVfm+HAc8DZgNH255bNlVEd0hayva8\n0jli0Ujaj+aUU4C/AL+0/ZOCkWIAMomOKETSjsDG7eUc2z8qmWcQJJ0LzAIuoTnlb0Xb7ywaaoDa\nTa4fAp5LBza5Rn36D+2QdLztD5TOFE9M0lLAp2jafP6mHV4b+CrwUduPlMoWE5dJdAw9SbuOM3wP\nzWlP6YowRCRdb3uzvusFTumqnaTraTa5zqJvk6vtWcVCxUiRdG3fKaCd+v/VRZI+B6wIfHDMSaef\nAR6wfWDJfDExqYmOGuwDvBj4cXv9MppJzDqSjrB9aqlg8dckzWD+qVxT+68rrmPv6dom16hPVr7q\n8hrg+e5bsbQ9ty3v+BmQSXTFMomOGiwFbNSrFZb0dOAUmg4QlwCZRA+PlWlucNQ31qv7NlBrHfuq\n7Zdd2+Qa9dlQ0g00/8fWa7+GyjvgdJg9ziN/249Kyg1R5TKJjhqsNWaz3e3t2F2SUk82RGw/t3SG\nJWQWzU1A7+bgw32vVXtzEFVKW7S63CRpL9un9A9K2pNmJToqlkl01OAiSefQHLoCsFs7tjzNLucY\nEpIWWp9pu9aWTm+1/d+lQ0TY/nXva0nPAda3/UNJ08nv9GH0PuAMSXvT3IwDvBCYTnPKZFQsGwtj\n6EkSsCuwXTv0E+A74z0ii7Ik9erWp9H8orieZvV2U+Bq2y8ulW0isoErho2k9wD7AqvaXk/S+sC/\n2d6pcLQYx5huTDfZvrBknhiM3LXG0LNtSVcD97QrLssBKwD3Fo4WY9jeAUDSGcCWtme31y8ADi8Y\nbaL05H8kYlK9D3gRcCWA7VvbPu0xhNoWptW3MY0FZRIdQ69/xQVYD3gWTZuxrLgMrw16E2gA2zdK\nqrmWcx1JZz3Ri7ZfN5lhIoCHbD/cPKh7vB9xns5FTKJMoqMGWXGpzw2STgROa6/fBtywkD8/7P4M\nfLZ0iIg+F0s6DJguaWdgf+DswpkiRkom0VGDrLjU513AfszvgXoJUHN/5XttX1w6RESfj9D00J8N\nvBf4L+DEookiRkwm0VGDrLhUxvaDkr4A/JDmhueWyo+3/VXpABH9bD8m6TTgEtu3lM4TMYrSnSOG\nXtud493ALjQbvM4DTkx3juEl6WXAyTSTTwFrAe+wfUnBWAMh6W+A59K3CDG2B2zEkibpdcA/A8vY\nXkfS5sARqc+PmDyZRMdQkzQVmGN7w9JZ4qmTNIumt/It7fXzga/b3qpssomRdCrN5tbrgEfbYds+\noFyqGEXt/7EdgYtsb9GOzba9SdlkEaMj5Rwx1NqjUW+RtLbt35TOE0/Z0v2PmG3/XNLSJQMNyAuB\nmXkKEkPgEdv39PaKtPJzGTGJMomOGswA5ki6CrivN5jHlkPt6nG6c1xdMM+g3Ag8A/hj6SAx8uZI\neiswtT1o5QDg8sKZIkZKyjli6El66Xjj6ZYwvCQtS9OasHfK5KXAF20/VC7VxLUnMm4OXAU8/r3k\nhi4mW3vo1Edp9opAs1fkKNsPlksVMVoyiY6hJWka8PfA82jaOH3F9ryyqeKpkrQMsAHd6M4B5IYu\nhkO7V+SHvRNCI6KMlHPEMDsZeIRmFfOVwEzm9x2OITZedw5J1XfnyGQ5hkG7V+QxSSvbvqd0nohR\nlZXoGFr9O83bA1ausr1l4VjxFHStO4eky2xvJ+leFty8JZruHCsVihYjStKZwBbABSy4VySdYiIm\nSVaiY5g9/vjf9rwxu9BjuHWqO4ft7drPK5bOEtE6o/2IiEKyEh1DS9KjzF9hETAduJ+s/g09SV8F\nHmPB7hxTbe9dLtXESdrH9lfGjB1j+yOlMkVERBlZiY6hZXtq6Qyx2Paj6c7Re7R8KfDFcnEG5k2S\nHrT9nwDt0ebTC2eKESRpNn/dF/oemlaSR9m+c/JTRYyWrERHRDxFkqYDZwFfBV4B/MV2NrvGpJN0\nLM2pmV9rh94CLAfcBmxn+7WlskWMikyiI2JgnmB17HG2N53EOAMjadW+yxWBM4HLgI8D2L6rRK4Y\nXZKuGbvRujeW478jJkfKOSJikF5TOsASMovm5kB9n1/VfgCsWyhXjK6pkl5k+yoASVsDvRK49NOP\nmARZiY6IJUrSasCdrvjNRtKLgN/a/mN7/Q7gTTR9sA/PSnRMtnbS/FVgBZqburnAu4E5wKttf6tg\nvIiRkEl0RAyMpG2BY4C7gCOBU4HVgCnAXrbPLRhvsUm6Bni57bskbQ98A/gAzRHgG9nerWjAGFmS\nVgbIoSsRky+T6IgYGElXA4cBKwNfAl5p+wpJG9IctrJF0YCLSdL1tjdrv/4C8Gfbh7fX19nevGS+\nGB2S9rR9mqSDx3vd9r9MdqaIUZWa6IgYpKVsnw8g6QjbVwDY/lnlh+VMlbSU7XnATsC+fa/lfTQm\n0/Lt5xz8E1FY3vwjYpAe6/v6gTGv1fzY6+vAxZLuoPm+LgWQ9Dya3rwRk8L2v7ef/6l0lohRl3KO\niBiYvlMm+0+YpL2eZrvao7/beu81gfNt39eOPR9YwfY1RcPFyJB03MJet33Awl6PiMHJSnREDEyX\nT5nslaaMGft5iSwx0ma1n18CzAS+2V7vDtxUJFHEiMpKdERERGUkXUFzMuG89npp4FLb25ZNFjE6\nppQOEBEREYtsBrBS3/UK7VhETJKUc0RERNTnGOBaST+m2XOwPXB40UQRIyblHBERERWS9Axgm/by\nStu3lcwTMWpSzhEREVEZNY3XXw5sZvtMYJn2ePqImCRZiY6IiKiMpBNo+rLvaHsjSTNo2i9uXTha\nxMhITXRERER9trG9paRrAWzfLWmZ0qEiRknKOSIiIurziKSptCeBSlqdBU8MjYglLJPoiIiI+hwH\nfBdYQ9IngcuAT5WNFDFaUhMdERFRIUkbAjvRtLi70PbNhSNFjJTUREdERFRC0jbAl4D1gNnAPrZz\n3HdEASnniIiIqMcXgA8BTwP+Bfhc2TgRoyuT6IiIiHpMsX2B7Ydsnw6sXjpQxKhKOUdEREQ9VpG0\n6xNd2z6jQKaIkZSNhREREZWQdNJCXrbtvSctTMSIyyQ6IiIiImIRpSY6IiKiMpIOlLSSGidKukbS\nLqVzRYySTKIjIiLqs7ftucAuNJ063g4cUzZSxGjJJDoiIqI+aj+/CjjF9py+sYiYBJlER0RE1GeW\npPNpJtHnSVoReKxwpoiRko2FERERlZE0Bdgc+IXtv0h6GvAs2zcUjhYxMrISHRERUR8DM4ED2uvl\ngWnl4kSMnqxER0REVEbSCTTlGzva3kjSDOB821sXjhYxMnJiYURERH22sb2lpGsBbN8taZnSoSJG\nSco5IiIi6vOIpKk0ZR1IWp1sLIyYVJlER0RE1Oc44LvAGpI+CVwGHF02UsRoSU10REREhSRtCOxE\n0x/6Qts3F44UMVIyiY6IiKiMpFNtv/3JxiJiyUk5R0RERH027r9o66O3KpQlYiRlEh0REVEJSYdK\nuhfYVNJcSfe217cDZxaOFzFSUs4RERFRGUlH2z60dI6IUZZJdERERGXaY7/fCqxj+0hJawFr2r6q\ncLSIkZFJdERERGVyYmFEeTmxMCIioj45sTCisGwsjIiIqE9OLIwoLJPoiIiI+vROLHx634mFnyob\nKWK0pCY6IiKiQn0nFgL8KCcWRkyu1ERHRETUaTmgV9IxvXCWiJGTco6IiIjKSPo4cDKwKrAacJKk\nj5VNFTFaUs4RERFRGUm3AJvZfrC9ng5cZ3uDsskiRkdWoiMiIurzB2Ba3/WywO8LZYkYSamJjoiI\nqISk42lqoO8B5ki6oL3eGchphRGTKOUcERERlZD0joW9bvvkycoSMeoyiY6IiIiIWEQp54iIiKiM\npPWBo4GZ9NVG2163WKiIEZONhREREfU5CTgBmAfsAJwCnFY0UcSISTlHREREZSTNsr2VpNm2N+kf\nK50tYlSknCMiIqI+D0maAtwq6f007e1WKJwpYqRkJToiIqIykrYGbgZWAY4EVgaOtX1F0WARIyST\n6IiIiIiIRZRyjoiIiEpI+rztgySdTXPIygJsv65ArIiRlEl0REREPU5tP3+maIqISDlHREREjSSt\nDmD7z6WzRIyi9ImOiIioiKTDJd0B3AL8XNKfJX28dK6IUZNJdERERCUkHQy8BNja9qq2ZwDbAC+R\n9MGy6SJGS8o5IiIiKiHpWmBn23eMGV8dON/2FmWSRYyerERHRETUY+mxE2h4vC566QJ5IkZWJtER\nERH1eHgxX4uIAUs5R0RERCUkPQrcN95LwDTbWY2OmCSZREdERERELKKUc0RERERELKJMoiMiIiIi\nFlEm0RERERERiyiT6IiIiIiIRZRJdERERETEIvr/2FRtIsgtwI4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 792x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aokKugy8fOEu",
        "colab_type": "text"
      },
      "source": [
        "It looks like there are some missing values replaced by \"0\" in this dataset. The minimum for some variables does not make sense if it is 0. Let's check the percentage of zeros for each columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LuLvUz7yiig",
        "colab_type": "code",
        "outputId": "6439dfbf-4037-4da7-8f09-7ce78e0b397e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(\"Percentage of Missing Values in each Colume\")\n",
        "for col in df.columns[:-1]:\n",
        "    print (col + \" {0:.0%}\".format(df[df[col] == 0].shape[0]/df.shape[0]))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of Missing Values in each Colume\n",
            "Pregnancies 14%\n",
            "Glucose 1%\n",
            "BloodPressure 5%\n",
            "SkinThickness 30%\n",
            "Insulin 49%\n",
            "BMI 1%\n",
            "DiabetesPedigreeFunction 0%\n",
            "Age 0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqCTLJSM8Nzz",
        "colab_type": "text"
      },
      "source": [
        "Let's substitute the zero values in some columns with median/mean by target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8vIqK9h8Z1L",
        "colab_type": "code",
        "outputId": "af62674d-82d2-419f-8aa1-01b6b8f1a9a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "df_new = df.copy()\n",
        "cols_to_change = ['BloodPressure', 'BMI', 'Insulin', 'SkinThickness',\n",
        "                  'Glucose']\n",
        "\n",
        "for col in cols_to_change:\n",
        "    mean_0 = np.mean(df.loc[(df.Outcome == 0) & (df[col] != 0) , col])\n",
        "    median_0 = np.median(df.loc[(df.Outcome == 0) & (df[col] != 0), col])\n",
        "    print(col, \"mean: \", mean_0, \"median: \", median_0)\n",
        "    mean_1 = np.mean(df.loc[(df.Outcome == 1) & (df[col] != 0), col])\n",
        "    median_1 = np.median(df.loc[(df.Outcome == 1) & (df[col] != 0), col])\n",
        "    print(col, \"mean: \", mean_1, \"median: \", median_1)\n",
        "\n",
        "    if col == 'Insulin':\n",
        "        df_new.loc[(df_new[col] == 0) & (df_new['Outcome'] == 0), col] = median_0\n",
        "        df_new.loc[(df_new[col] == 0) & (df_new['Outcome'] == 1), col] = median_1\n",
        "    else:\n",
        "        df_new.loc[(df_new[col] == 0) & (df_new['Outcome'] == 0), col] = mean_0\n",
        "        df_new.loc[(df_new[col] == 0) & (df_new['Outcome'] == 1), col] = mean_1"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BloodPressure mean:  70.87733887733887 median:  70.0\n",
            "BloodPressure mean:  75.32142857142857 median:  74.5\n",
            "BMI mean:  30.85967413441951 median:  30.1\n",
            "BMI mean:  35.40676691729318 median:  34.3\n",
            "Insulin mean:  130.28787878787878 median:  102.5\n",
            "Insulin mean:  206.84615384615384 median:  169.5\n",
            "SkinThickness mean:  27.235457063711912 median:  27.0\n",
            "SkinThickness mean:  33.0 median:  32.0\n",
            "Glucose mean:  110.64386317907444 median:  107.0\n",
            "Glucose mean:  142.31954887218046 median:  140.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTH3vN_seNBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_new.loc[(df_new.Pregnancies > np.percentile(df_new.Pregnancies, 75)),\n",
        "#            'Pregnancies'] = np.percentile(df_new.Pregnancies, 75)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwSVIGRKSkjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.model_selection import KFold\n",
        "\n",
        "# import tensorflow.contrib.eager as tfe\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "# y = df_new.Outcome\n",
        "# def make_dataset(X_data,y_data,n_splits):\n",
        "\n",
        "#     def gen():\n",
        "#         for train_index, test_index in KFold(n_splits).split(X_data):\n",
        "#             X_train, X_test = X_data[train_index], X_data[test_index]\n",
        "#             y_train, y_test = y_data[train_index], y_data[test_index]\n",
        "#             yield X_train,y_train,X_test,y_test\n",
        "\n",
        "#     return tf.data.Dataset.from_generator(gen, (tf.float64,tf.float64,tf.float64,tf.float64))\n",
        "\n",
        "# dataset=make_dataset(df_new, y, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHMR566-haoN",
        "colab_type": "text"
      },
      "source": [
        "# Creating Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ254XGnT8n4",
        "colab_type": "text"
      },
      "source": [
        "First, let's separate the dataset into training and test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtToqnP6UFLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " y = df_new.Outcome #just for the splitting sakes\n",
        " X_train, X_test, y_train, y_test = train_test_split(df_new, y, test_size=0.2, \n",
        "                                                      random_state=42, \n",
        "                                                     shuffle = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwl5B7iqd61A",
        "colab_type": "text"
      },
      "source": [
        "Let's put the training set into tensorflow frame.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iViwILvI6iVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an empty list that will eventually hold all created feature columns.\n",
        "dataset_columns = df_new.columns\n",
        "feature_columns = []\n",
        "\n",
        "# Create numerical feature columns to represent all columns in dataset\n",
        "for col in dataset_columns[:-1]: \n",
        "    #Excluding outcome value, since outcome is still included in dataset\n",
        "    col_tf = tf.feature_column.numeric_column(col)\n",
        "    feature_columns.append(col_tf)\n",
        "\n",
        "# Convert the list of feature columns into a layer that will later be fed into\n",
        "# the model. \n",
        "feature_layer = layers.DenseFeatures(feature_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4G8oBK_ciALS",
        "colab_type": "text"
      },
      "source": [
        "Next, create the layers and fit the dataset into the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcQYLoEb7n96",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def create_model(learningRate, featureLayer, metrics):\n",
        "  \"\"\"Construct the model by passing dataset through regression & activation\"\"\"\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add the feature layer that consists the columns/features of the dataset\n",
        "  model.add(featureLayer)\n",
        "\n",
        "  # Going through regression and then the sigmoid function for activation\n",
        "  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,),\n",
        "                                  activation=tf.sigmoid),)\n",
        "\n",
        "  # Optimise to get the best result, represented by the metrics\n",
        "  # Use binary cross entropy loss due to binary classification case\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr=learningRate),                                                   \n",
        "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                metrics=metrics)\n",
        "\n",
        "  return model        \n",
        "\n",
        "\n",
        "def train_model(model, features, label, epochs, label_name,\n",
        "                batch_size=None, shuffle=True):\n",
        "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
        "\n",
        "  # Fit the data into the model\n",
        "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                      epochs=epochs, shuffle=shuffle, verbose=1)\n",
        "\n",
        "  return history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLVlyJBiRSf",
        "colab_type": "text"
      },
      "source": [
        "Create Plot to visualise the result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw1s6lvE8P1b",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "def plot_curve(epochs, hist, list_of_metrics):\n",
        "  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Value\")\n",
        "\n",
        "  for m in list_of_metrics:\n",
        "    x = hist[m]\n",
        "    plt.plot(epochs[1:], x[1:], label=m)\n",
        "\n",
        "  plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_KVl81Vid6K",
        "colab_type": "text"
      },
      "source": [
        "Now, let's fit the train set into the model. This time, dataset will be split by k-fold, in which there will be k pairs of train and test sets to model and evaluate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBU7YK1sUYzM",
        "colab_type": "code",
        "outputId": "adb8e38b-9d72-420f-8add-7110c3f62e5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.001\n",
        "epochs = 700\n",
        "batch_size = 10\n",
        "label_name = \"Outcome\"\n",
        "classification_threshold = 0.4\n",
        "\n",
        "# Establish the metrics the model will measure.\n",
        "METRICS = [\n",
        "           tf.keras.metrics.BinaryAccuracy(name='accuracy', \n",
        "                                           threshold=classification_threshold),\n",
        "           tf.keras.metrics.Precision(thresholds=classification_threshold,\n",
        "                                 name='precision' \n",
        "                                 ),\n",
        "           tf.keras.metrics.Recall(thresholds=classification_threshold,\n",
        "                                 name='recall' \n",
        "                                 ),\n",
        "          ]\n",
        "\n",
        "# Train the model with K-Fold = 3\n",
        "kf = KFold(3, random_state=2020, shuffle = True)\n",
        "batch = 1\n",
        "for train_idx, val_idx in kf.split(df_new, y):\n",
        "\n",
        "    # Select the part of dataset to be train set-test set\n",
        "    # according to indexes from K-Fold\n",
        "    train_x = df_new.iloc[train_idx]\n",
        "    train_y = y[train_idx].to_numpy()\n",
        "    val_x = df_new.iloc[val_idx]\n",
        "    val_y = y[val_idx].to_numpy()\n",
        "\n",
        "    # Construct the model\n",
        "    my_model = create_model(learning_rate, feature_layer, METRICS)\n",
        "\n",
        "    # Converting train set into numpy formats\n",
        "    train_features = {name:np.array(value) for name, value in train_x.items()}\n",
        "    # Remove the label from the features\n",
        "    train_label = train_y\n",
        "    # Fit the data into the model\n",
        "    history = train_model(my_model, train_features, train_label, \n",
        "                          epochs, label_name, batch_size)\n",
        "    \n",
        "    # Let's see the training result of this batch:\n",
        "    train_res = my_model.evaluate(x=train_features, y=train_label, batch_size=batch_size)\n",
        "    print(\"Train result:\")\n",
        "    print(\"Batch \" + str(batch) +  \" accuracy: \" + \"{:.00%}%\".format(train_res[1]))\n",
        "    print(\"Batch \" + str(batch) +  \" precision: \" + \"{:.00%}%\".format(train_res[2]))\n",
        "    print(\"Batch \" + str(batch) +  \" recall: \" + \"{:.00%}%\".format(train_res[3]))\n",
        "        \n",
        "    # Let's see how it goes with the test set for this batch:\n",
        "    test_features = {name:np.array(value) for name, value in val_x.drop(['Outcome'], axis = 1).items()}\n",
        "    test_label = val_y \n",
        "    test_res = my_model.evaluate(x = test_features, y = test_label, batch_size=batch_size)\n",
        "\n",
        "    print(\"Test result:\")\n",
        "    print(\"Batch \" + str(batch) +  \" accuracy: \" + \"{:.00%}%\".format(test_res[1]))\n",
        "    print(\"Batch \" + str(batch) +  \" precision: \" + \"{:.00%}%\".format(test_res[2]))\n",
        "    print(\"Batch \" + str(batch) +  \" recall: \" + \"{:.00%}%\".format(test_res[3]))\n",
        "    \n",
        "    # # The list of epochs is stored separately from the rest of history.\n",
        "    # epochs = history.epoch\n",
        "\n",
        "    # # Isolate the classification metric for each epoch.\n",
        "    # hist = pd.DataFrame(history.history)\n",
        "    \n",
        "    # plot_curve(epochs, hist, METRICS)\n",
        "    batch += 1 "
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 39.4964 - accuracy: 0.3418 - precision: 0.3418 - recall: 1.0000\n",
            "Epoch 2/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 27.4154 - accuracy: 0.3418 - precision: 0.3418 - recall: 1.0000\n",
            "Epoch 3/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 14.5350 - accuracy: 0.3418 - precision: 0.3418 - recall: 1.0000\n",
            "Epoch 4/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 4.1385 - accuracy: 0.4551 - precision: 0.3632 - recall: 0.7886\n",
            "Epoch 5/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 2.4326 - accuracy: 0.5449 - precision: 0.3657 - recall: 0.4514\n",
            "Epoch 6/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 2.2666 - accuracy: 0.5527 - precision: 0.3805 - recall: 0.4914\n",
            "Epoch 7/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 2.1388 - accuracy: 0.5684 - precision: 0.4000 - recall: 0.5257\n",
            "Epoch 8/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 2.1664 - accuracy: 0.5684 - precision: 0.3925 - recall: 0.4800\n",
            "Epoch 9/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 2.0256 - accuracy: 0.5781 - precision: 0.4072 - recall: 0.5143\n",
            "Epoch 10/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 1.8710 - accuracy: 0.5762 - precision: 0.4087 - recall: 0.5371\n",
            "Epoch 11/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.7828 - accuracy: 0.6016 - precision: 0.4338 - recall: 0.5429\n",
            "Epoch 12/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.7276 - accuracy: 0.6074 - precision: 0.4363 - recall: 0.5086\n",
            "Epoch 13/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.6766 - accuracy: 0.6152 - precision: 0.4481 - recall: 0.5429\n",
            "Epoch 14/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.7068 - accuracy: 0.6172 - precision: 0.4502 - recall: 0.5429\n",
            "Epoch 15/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.6027 - accuracy: 0.6406 - precision: 0.4776 - recall: 0.5486\n",
            "Epoch 16/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.6016 - accuracy: 0.6348 - precision: 0.4720 - recall: 0.5771\n",
            "Epoch 17/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.6559 - accuracy: 0.6328 - precision: 0.4677 - recall: 0.5371\n",
            "Epoch 18/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.4988 - accuracy: 0.6484 - precision: 0.4874 - recall: 0.5543\n",
            "Epoch 19/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.5022 - accuracy: 0.6348 - precision: 0.4694 - recall: 0.5257\n",
            "Epoch 20/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.5048 - accuracy: 0.6465 - precision: 0.4854 - recall: 0.5714\n",
            "Epoch 21/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 1.4201 - accuracy: 0.6523 - precision: 0.4929 - recall: 0.5943\n",
            "Epoch 22/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 1.3980 - accuracy: 0.6426 - precision: 0.4813 - recall: 0.5886\n",
            "Epoch 23/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.3787 - accuracy: 0.6758 - precision: 0.5251 - recall: 0.5371\n",
            "Epoch 24/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 1.3279 - accuracy: 0.6504 - precision: 0.4903 - recall: 0.5771\n",
            "Epoch 25/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.3031 - accuracy: 0.6680 - precision: 0.5131 - recall: 0.5600\n",
            "Epoch 26/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 1.2419 - accuracy: 0.6660 - precision: 0.5104 - recall: 0.5600\n",
            "Epoch 27/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.2381 - accuracy: 0.6445 - precision: 0.4836 - recall: 0.5886\n",
            "Epoch 28/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 1.2068 - accuracy: 0.6719 - precision: 0.5183 - recall: 0.5657\n",
            "Epoch 29/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.2159 - accuracy: 0.6777 - precision: 0.5269 - recall: 0.5600\n",
            "Epoch 30/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 1.1520 - accuracy: 0.6660 - precision: 0.5097 - recall: 0.6000\n",
            "Epoch 31/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.1347 - accuracy: 0.6660 - precision: 0.5109 - recall: 0.5371\n",
            "Epoch 32/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.1290 - accuracy: 0.6641 - precision: 0.5075 - recall: 0.5829\n",
            "Epoch 33/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.1340 - accuracy: 0.6699 - precision: 0.5158 - recall: 0.5600\n",
            "Epoch 34/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.0730 - accuracy: 0.6699 - precision: 0.5156 - recall: 0.5657\n",
            "Epoch 35/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.0735 - accuracy: 0.6758 - precision: 0.5224 - recall: 0.6000\n",
            "Epoch 36/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.1117 - accuracy: 0.6719 - precision: 0.5193 - recall: 0.5371\n",
            "Epoch 37/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 1.0243 - accuracy: 0.6680 - precision: 0.5137 - recall: 0.5371\n",
            "Epoch 38/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.9943 - accuracy: 0.6621 - precision: 0.5050 - recall: 0.5771\n",
            "Epoch 39/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.9884 - accuracy: 0.6777 - precision: 0.5266 - recall: 0.5657\n",
            "Epoch 40/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.9725 - accuracy: 0.6777 - precision: 0.5258 - recall: 0.5829\n",
            "Epoch 41/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.9659 - accuracy: 0.6855 - precision: 0.5368 - recall: 0.5829\n",
            "Epoch 42/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.9285 - accuracy: 0.6934 - precision: 0.5474 - recall: 0.5943\n",
            "Epoch 43/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.9144 - accuracy: 0.6914 - precision: 0.5415 - recall: 0.6343\n",
            "Epoch 44/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.9328 - accuracy: 0.6699 - precision: 0.5156 - recall: 0.5657\n",
            "Epoch 45/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.9546 - accuracy: 0.6758 - precision: 0.5257 - recall: 0.5257\n",
            "Epoch 46/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.8738 - accuracy: 0.6875 - precision: 0.5393 - recall: 0.5886\n",
            "Epoch 47/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.9416 - accuracy: 0.6660 - precision: 0.5101 - recall: 0.5771\n",
            "Epoch 48/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.8527 - accuracy: 0.6816 - precision: 0.5303 - recall: 0.6000\n",
            "Epoch 49/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.8490 - accuracy: 0.6914 - precision: 0.5423 - recall: 0.6229\n",
            "Epoch 50/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.8163 - accuracy: 0.6934 - precision: 0.5474 - recall: 0.5943\n",
            "Epoch 51/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.8109 - accuracy: 0.6895 - precision: 0.5421 - recall: 0.5886\n",
            "Epoch 52/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.8363 - accuracy: 0.6914 - precision: 0.5440 - recall: 0.6000\n",
            "Epoch 53/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.8051 - accuracy: 0.7012 - precision: 0.5539 - recall: 0.6457\n",
            "Epoch 54/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.8024 - accuracy: 0.6895 - precision: 0.5388 - recall: 0.6343\n",
            "Epoch 55/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.7609 - accuracy: 0.6973 - precision: 0.5532 - recall: 0.5943\n",
            "Epoch 56/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.8026 - accuracy: 0.6973 - precision: 0.5500 - recall: 0.6286\n",
            "Epoch 57/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.7310 - accuracy: 0.7090 - precision: 0.5631 - recall: 0.6629\n",
            "Epoch 58/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.7698 - accuracy: 0.6836 - precision: 0.5333 - recall: 0.5943\n",
            "Epoch 59/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.7284 - accuracy: 0.6836 - precision: 0.5337 - recall: 0.5886\n",
            "Epoch 60/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.7374 - accuracy: 0.6973 - precision: 0.5538 - recall: 0.5886\n",
            "Epoch 61/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.7135 - accuracy: 0.7012 - precision: 0.5579 - recall: 0.6057\n",
            "Epoch 62/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.7002 - accuracy: 0.7012 - precision: 0.5598 - recall: 0.5886\n",
            "Epoch 63/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.7109 - precision: 0.5678 - recall: 0.6457\n",
            "Epoch 64/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.7149 - accuracy: 0.7051 - precision: 0.5619 - recall: 0.6229\n",
            "Epoch 65/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6835 - accuracy: 0.7070 - precision: 0.5635 - recall: 0.6343\n",
            "Epoch 66/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6690 - accuracy: 0.7031 - precision: 0.5622 - recall: 0.5943\n",
            "Epoch 67/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.6560 - accuracy: 0.7109 - precision: 0.5754 - recall: 0.5886\n",
            "Epoch 68/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6450 - accuracy: 0.7266 - precision: 0.5845 - recall: 0.6914\n",
            "Epoch 69/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6793 - accuracy: 0.6895 - precision: 0.5435 - recall: 0.5714\n",
            "Epoch 70/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.6329 - accuracy: 0.7012 - precision: 0.5561 - recall: 0.6229\n",
            "Epoch 71/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6487 - accuracy: 0.7207 - precision: 0.5816 - recall: 0.6514\n",
            "Epoch 72/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.6193 - accuracy: 0.7148 - precision: 0.5736 - recall: 0.6457\n",
            "Epoch 73/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6166 - accuracy: 0.6914 - precision: 0.5436 - recall: 0.6057\n",
            "Epoch 74/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.6136 - accuracy: 0.6973 - precision: 0.5500 - recall: 0.6286\n",
            "Epoch 75/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6256 - accuracy: 0.6973 - precision: 0.5500 - recall: 0.6286\n",
            "Epoch 76/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6034 - accuracy: 0.7109 - precision: 0.5665 - recall: 0.6571\n",
            "Epoch 77/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6252 - accuracy: 0.7070 - precision: 0.5628 - recall: 0.6400\n",
            "Epoch 78/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.6362 - accuracy: 0.6855 - precision: 0.5310 - recall: 0.6857\n",
            "Epoch 79/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.6262 - accuracy: 0.6953 - precision: 0.5497 - recall: 0.6000\n",
            "Epoch 80/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6117 - accuracy: 0.7129 - precision: 0.5714 - recall: 0.6400\n",
            "Epoch 81/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.6384 - accuracy: 0.7090 - precision: 0.5677 - recall: 0.6229\n",
            "Epoch 82/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.6078 - accuracy: 0.6953 - precision: 0.5549 - recall: 0.5486\n",
            "Epoch 83/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5839 - accuracy: 0.7207 - precision: 0.5851 - recall: 0.6286\n",
            "Epoch 84/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5891 - accuracy: 0.6992 - precision: 0.5502 - recall: 0.6571\n",
            "Epoch 85/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5892 - accuracy: 0.6895 - precision: 0.5388 - recall: 0.6343\n",
            "Epoch 86/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5684 - accuracy: 0.7090 - precision: 0.5657 - recall: 0.6400\n",
            "Epoch 87/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5920 - accuracy: 0.6934 - precision: 0.5446 - recall: 0.6286\n",
            "Epoch 88/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5758 - accuracy: 0.6875 - precision: 0.5373 - recall: 0.6171\n",
            "Epoch 89/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5598 - accuracy: 0.7227 - precision: 0.5864 - recall: 0.6400\n",
            "Epoch 90/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5756 - accuracy: 0.7207 - precision: 0.5842 - recall: 0.6343\n",
            "Epoch 91/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5628 - accuracy: 0.7266 - precision: 0.5829 - recall: 0.7029\n",
            "Epoch 92/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5577 - accuracy: 0.7109 - precision: 0.5672 - recall: 0.6514\n",
            "Epoch 93/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5580 - accuracy: 0.7051 - precision: 0.5594 - recall: 0.6457\n",
            "Epoch 94/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5573 - accuracy: 0.7148 - precision: 0.5819 - recall: 0.5886\n",
            "Epoch 95/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5768 - accuracy: 0.7070 - precision: 0.5668 - recall: 0.6057\n",
            "Epoch 96/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5525 - accuracy: 0.6934 - precision: 0.5433 - recall: 0.6457\n",
            "Epoch 97/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5501 - accuracy: 0.7148 - precision: 0.5729 - recall: 0.6514\n",
            "Epoch 98/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5892 - accuracy: 0.7031 - precision: 0.5567 - recall: 0.6457\n",
            "Epoch 99/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5549 - accuracy: 0.7227 - precision: 0.5838 - recall: 0.6571\n",
            "Epoch 100/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5596 - accuracy: 0.7188 - precision: 0.5771 - recall: 0.6629\n",
            "Epoch 101/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5443 - accuracy: 0.7441 - precision: 0.6170 - recall: 0.6629\n",
            "Epoch 102/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5699 - accuracy: 0.7090 - precision: 0.5670 - recall: 0.6286\n",
            "Epoch 103/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5547 - accuracy: 0.7285 - precision: 0.5947 - recall: 0.6457\n",
            "Epoch 104/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5607 - accuracy: 0.7051 - precision: 0.5577 - recall: 0.6629\n",
            "Epoch 105/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5472 - accuracy: 0.7207 - precision: 0.5860 - recall: 0.6229\n",
            "Epoch 106/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5473 - accuracy: 0.7168 - precision: 0.5735 - recall: 0.6686\n",
            "Epoch 107/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5559 - accuracy: 0.7012 - precision: 0.5585 - recall: 0.6000\n",
            "Epoch 108/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5608 - accuracy: 0.7070 - precision: 0.5714 - recall: 0.5714\n",
            "Epoch 109/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5778 - accuracy: 0.7051 - precision: 0.5561 - recall: 0.6800\n",
            "Epoch 110/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.7305 - precision: 0.6000 - recall: 0.6343\n",
            "Epoch 111/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5416 - accuracy: 0.7422 - precision: 0.6029 - recall: 0.7200\n",
            "Epoch 112/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5550 - accuracy: 0.7148 - precision: 0.5751 - recall: 0.6343\n",
            "Epoch 113/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5416 - accuracy: 0.7227 - precision: 0.5864 - recall: 0.6400\n",
            "Epoch 114/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5396 - accuracy: 0.7168 - precision: 0.5735 - recall: 0.6686\n",
            "Epoch 115/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5371 - accuracy: 0.7344 - precision: 0.6000 - recall: 0.6686\n",
            "Epoch 116/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5353 - accuracy: 0.7266 - precision: 0.5879 - recall: 0.6686\n",
            "Epoch 117/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5408 - accuracy: 0.7207 - precision: 0.5842 - recall: 0.6343\n",
            "Epoch 118/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7207 - precision: 0.5741 - recall: 0.7086\n",
            "Epoch 119/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5429 - accuracy: 0.7383 - precision: 0.6030 - recall: 0.6857\n",
            "Epoch 120/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5247 - accuracy: 0.7285 - precision: 0.5918 - recall: 0.6629\n",
            "Epoch 121/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5296 - accuracy: 0.7109 - precision: 0.5699 - recall: 0.6286\n",
            "Epoch 122/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5262 - accuracy: 0.7285 - precision: 0.5957 - recall: 0.6400\n",
            "Epoch 123/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5271 - accuracy: 0.7402 - precision: 0.6040 - recall: 0.6971\n",
            "Epoch 124/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5306 - accuracy: 0.7305 - precision: 0.6000 - recall: 0.6343\n",
            "Epoch 125/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5304 - accuracy: 0.7324 - precision: 0.5979 - recall: 0.6629\n",
            "Epoch 126/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5397 - accuracy: 0.7188 - precision: 0.5771 - recall: 0.6629\n",
            "Epoch 127/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5283 - accuracy: 0.7383 - precision: 0.6030 - recall: 0.6857\n",
            "Epoch 128/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5881 - accuracy: 0.7227 - precision: 0.5864 - recall: 0.6400\n",
            "Epoch 129/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5977 - accuracy: 0.6680 - precision: 0.5134 - recall: 0.5486\n",
            "Epoch 130/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5370 - accuracy: 0.7266 - precision: 0.5879 - recall: 0.6686\n",
            "Epoch 131/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5215 - accuracy: 0.7305 - precision: 0.5949 - recall: 0.6629\n",
            "Epoch 132/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5321 - accuracy: 0.7246 - precision: 0.5934 - recall: 0.6171\n",
            "Epoch 133/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5249 - accuracy: 0.7461 - precision: 0.6087 - recall: 0.7200\n",
            "Epoch 134/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5367 - accuracy: 0.7266 - precision: 0.5879 - recall: 0.6686\n",
            "Epoch 135/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5472 - accuracy: 0.7246 - precision: 0.6012 - recall: 0.5771\n",
            "Epoch 136/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5418 - accuracy: 0.7246 - precision: 0.5833 - recall: 0.6800\n",
            "Epoch 137/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5361 - accuracy: 0.7363 - precision: 0.6124 - recall: 0.6229\n",
            "Epoch 138/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5606 - accuracy: 0.7148 - precision: 0.5714 - recall: 0.6629\n",
            "Epoch 139/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5357 - accuracy: 0.7324 - precision: 0.5905 - recall: 0.7086\n",
            "Epoch 140/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5371 - accuracy: 0.7246 - precision: 0.5825 - recall: 0.6857\n",
            "Epoch 141/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5452 - accuracy: 0.7148 - precision: 0.5784 - recall: 0.6114\n",
            "Epoch 142/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5280 - accuracy: 0.7402 - precision: 0.6129 - recall: 0.6514\n",
            "Epoch 143/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5351 - accuracy: 0.7285 - precision: 0.5938 - recall: 0.6514\n",
            "Epoch 144/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5339 - accuracy: 0.7520 - precision: 0.6212 - recall: 0.7029\n",
            "Epoch 145/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5286 - accuracy: 0.7227 - precision: 0.5855 - recall: 0.6457\n",
            "Epoch 146/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5503 - accuracy: 0.7246 - precision: 0.5904 - recall: 0.6343\n",
            "Epoch 147/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5472 - accuracy: 0.7188 - precision: 0.5866 - recall: 0.6000\n",
            "Epoch 148/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5364 - accuracy: 0.7461 - precision: 0.6216 - recall: 0.6571\n",
            "Epoch 149/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.7559 - precision: 0.6263 - recall: 0.7086\n",
            "Epoch 150/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5469 - accuracy: 0.7520 - precision: 0.6290 - recall: 0.6686\n",
            "Epoch 151/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5392 - accuracy: 0.6973 - precision: 0.5510 - recall: 0.6171\n",
            "Epoch 152/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5251 - accuracy: 0.7363 - precision: 0.5971 - recall: 0.7029\n",
            "Epoch 153/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5279 - accuracy: 0.7422 - precision: 0.6138 - recall: 0.6629\n",
            "Epoch 154/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5349 - accuracy: 0.7324 - precision: 0.6011 - recall: 0.6457\n",
            "Epoch 155/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5459 - accuracy: 0.7109 - precision: 0.5738 - recall: 0.6000\n",
            "Epoch 156/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5273 - accuracy: 0.7422 - precision: 0.6103 - recall: 0.6800\n",
            "Epoch 157/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5402 - accuracy: 0.7285 - precision: 0.5909 - recall: 0.6686\n",
            "Epoch 158/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5151 - accuracy: 0.7363 - precision: 0.5990 - recall: 0.6914\n",
            "Epoch 159/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5290 - accuracy: 0.7266 - precision: 0.5907 - recall: 0.6514\n",
            "Epoch 160/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5267 - accuracy: 0.7480 - precision: 0.6117 - recall: 0.7200\n",
            "Epoch 161/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.7598 - precision: 0.6354 - recall: 0.6971\n",
            "Epoch 162/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5313 - accuracy: 0.7266 - precision: 0.5967 - recall: 0.6171\n",
            "Epoch 163/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5399 - accuracy: 0.7402 - precision: 0.6082 - recall: 0.6743\n",
            "Epoch 164/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5154 - accuracy: 0.7305 - precision: 0.5989 - recall: 0.6400\n",
            "Epoch 165/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5202 - accuracy: 0.7344 - precision: 0.6021 - recall: 0.6571\n",
            "Epoch 166/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5247 - accuracy: 0.7285 - precision: 0.5968 - recall: 0.6343\n",
            "Epoch 167/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.7383 - precision: 0.6041 - recall: 0.6800\n",
            "Epoch 168/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5382 - accuracy: 0.7500 - precision: 0.6218 - recall: 0.6857\n",
            "Epoch 169/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5312 - accuracy: 0.7461 - precision: 0.6190 - recall: 0.6686\n",
            "Epoch 170/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5181 - accuracy: 0.7422 - precision: 0.6049 - recall: 0.7086\n",
            "Epoch 171/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5302 - accuracy: 0.7324 - precision: 0.5979 - recall: 0.6629\n",
            "Epoch 172/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5294 - accuracy: 0.7344 - precision: 0.5961 - recall: 0.6914\n",
            "Epoch 173/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5193 - accuracy: 0.7500 - precision: 0.6270 - recall: 0.6629\n",
            "Epoch 174/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5403 - accuracy: 0.7188 - precision: 0.5847 - recall: 0.6114\n",
            "Epoch 175/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5181 - accuracy: 0.7422 - precision: 0.6162 - recall: 0.6514\n",
            "Epoch 176/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5326 - accuracy: 0.7168 - precision: 0.5750 - recall: 0.6571\n",
            "Epoch 177/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7422 - precision: 0.6080 - recall: 0.6914\n",
            "Epoch 178/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5464 - accuracy: 0.7266 - precision: 0.5978 - recall: 0.6114\n",
            "Epoch 179/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5214 - accuracy: 0.7461 - precision: 0.6142 - recall: 0.6914\n",
            "Epoch 180/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5324 - accuracy: 0.7480 - precision: 0.6264 - recall: 0.6514\n",
            "Epoch 181/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.7461 - precision: 0.6243 - recall: 0.6457\n",
            "Epoch 182/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5190 - accuracy: 0.7520 - precision: 0.6277 - recall: 0.6743\n",
            "Epoch 183/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7578 - precision: 0.6269 - recall: 0.7200\n",
            "Epoch 184/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5151 - accuracy: 0.7539 - precision: 0.6207 - recall: 0.7200\n",
            "Epoch 185/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5241 - accuracy: 0.7266 - precision: 0.5956 - recall: 0.6229\n",
            "Epoch 186/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.7383 - precision: 0.6073 - recall: 0.6629\n",
            "Epoch 187/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5333 - accuracy: 0.7266 - precision: 0.5967 - recall: 0.6171\n",
            "Epoch 188/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5180 - accuracy: 0.7441 - precision: 0.6122 - recall: 0.6857\n",
            "Epoch 189/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5162 - accuracy: 0.7344 - precision: 0.6000 - recall: 0.6686\n",
            "Epoch 190/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5119 - accuracy: 0.7461 - precision: 0.6203 - recall: 0.6629\n",
            "Epoch 191/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5253 - accuracy: 0.7246 - precision: 0.5885 - recall: 0.6457\n",
            "Epoch 192/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5360 - accuracy: 0.7520 - precision: 0.6333 - recall: 0.6514\n",
            "Epoch 193/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5246 - accuracy: 0.7383 - precision: 0.6051 - recall: 0.6743\n",
            "Epoch 194/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5231 - accuracy: 0.7637 - precision: 0.6552 - recall: 0.6514\n",
            "Epoch 195/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5120 - accuracy: 0.7520 - precision: 0.6188 - recall: 0.7143\n",
            "Epoch 196/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.7461 - precision: 0.6166 - recall: 0.6800\n",
            "Epoch 197/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5205 - accuracy: 0.7422 - precision: 0.6229 - recall: 0.6229\n",
            "Epoch 198/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5141 - accuracy: 0.7344 - precision: 0.5942 - recall: 0.7029\n",
            "Epoch 199/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5207 - accuracy: 0.7305 - precision: 0.6034 - recall: 0.6171\n",
            "Epoch 200/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5203 - accuracy: 0.7344 - precision: 0.6054 - recall: 0.6400\n",
            "Epoch 201/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5189 - accuracy: 0.7441 - precision: 0.6196 - recall: 0.6514\n",
            "Epoch 202/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.7578 - precision: 0.6294 - recall: 0.7086\n",
            "Epoch 203/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5251 - accuracy: 0.7207 - precision: 0.5825 - recall: 0.6457\n",
            "Epoch 204/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5149 - accuracy: 0.7617 - precision: 0.6448 - recall: 0.6743\n",
            "Epoch 205/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5144 - accuracy: 0.7539 - precision: 0.6231 - recall: 0.7086\n",
            "Epoch 206/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5189 - accuracy: 0.7344 - precision: 0.6010 - recall: 0.6629\n",
            "Epoch 207/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5182 - accuracy: 0.7324 - precision: 0.6080 - recall: 0.6114\n",
            "Epoch 208/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5160 - accuracy: 0.7500 - precision: 0.6193 - recall: 0.6971\n",
            "Epoch 209/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5188 - accuracy: 0.7461 - precision: 0.6216 - recall: 0.6571\n",
            "Epoch 210/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5147 - accuracy: 0.7617 - precision: 0.6373 - recall: 0.7029\n",
            "Epoch 211/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.7480 - precision: 0.6223 - recall: 0.6686\n",
            "Epoch 212/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5432 - accuracy: 0.7188 - precision: 0.5812 - recall: 0.6343\n",
            "Epoch 213/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5410 - accuracy: 0.7285 - precision: 0.6216 - recall: 0.5257\n",
            "Epoch 214/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5113 - accuracy: 0.7578 - precision: 0.6256 - recall: 0.7257\n",
            "Epoch 215/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7520 - precision: 0.6224 - recall: 0.6971\n",
            "Epoch 216/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7578 - precision: 0.6308 - recall: 0.7029\n",
            "Epoch 217/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5159 - accuracy: 0.7402 - precision: 0.6154 - recall: 0.6400\n",
            "Epoch 218/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5196 - accuracy: 0.7617 - precision: 0.6221 - recall: 0.7714\n",
            "Epoch 219/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5408 - accuracy: 0.7285 - precision: 0.6059 - recall: 0.5886\n",
            "Epoch 220/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5115 - accuracy: 0.7480 - precision: 0.6198 - recall: 0.6800\n",
            "Epoch 221/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7246 - precision: 0.5924 - recall: 0.6229\n",
            "Epoch 222/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5131 - accuracy: 0.7480 - precision: 0.6173 - recall: 0.6914\n",
            "Epoch 223/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5235 - accuracy: 0.7480 - precision: 0.6085 - recall: 0.7371\n",
            "Epoch 224/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5353 - accuracy: 0.7363 - precision: 0.6282 - recall: 0.5600\n",
            "Epoch 225/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5260 - accuracy: 0.7168 - precision: 0.5750 - recall: 0.6571\n",
            "Epoch 226/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5392 - accuracy: 0.7168 - precision: 0.5708 - recall: 0.6914\n",
            "Epoch 227/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5288 - accuracy: 0.7188 - precision: 0.5847 - recall: 0.6114\n",
            "Epoch 228/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5021 - accuracy: 0.7676 - precision: 0.6359 - recall: 0.7486\n",
            "Epoch 229/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.7344 - precision: 0.6127 - recall: 0.6057\n",
            "Epoch 230/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5140 - accuracy: 0.7461 - precision: 0.6216 - recall: 0.6571\n",
            "Epoch 231/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5216 - accuracy: 0.7715 - precision: 0.6559 - recall: 0.6971\n",
            "Epoch 232/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5124 - accuracy: 0.7852 - precision: 0.6633 - recall: 0.7543\n",
            "Epoch 233/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5177 - accuracy: 0.7500 - precision: 0.6270 - recall: 0.6629\n",
            "Epoch 234/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5153 - accuracy: 0.7520 - precision: 0.6263 - recall: 0.6800\n",
            "Epoch 235/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5291 - accuracy: 0.7266 - precision: 0.5936 - recall: 0.6343\n",
            "Epoch 236/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5194 - accuracy: 0.7285 - precision: 0.5891 - recall: 0.6800\n",
            "Epoch 237/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5146 - accuracy: 0.7637 - precision: 0.6500 - recall: 0.6686\n",
            "Epoch 238/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7344 - precision: 0.5980 - recall: 0.6800\n",
            "Epoch 239/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5286 - accuracy: 0.7676 - precision: 0.6429 - recall: 0.7200\n",
            "Epoch 240/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5131 - accuracy: 0.7500 - precision: 0.6181 - recall: 0.7029\n",
            "Epoch 241/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5284 - accuracy: 0.7637 - precision: 0.6436 - recall: 0.6914\n",
            "Epoch 242/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5144 - accuracy: 0.7383 - precision: 0.5990 - recall: 0.7086\n",
            "Epoch 243/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5030 - accuracy: 0.7480 - precision: 0.6223 - recall: 0.6686\n",
            "Epoch 244/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5248 - accuracy: 0.7539 - precision: 0.6339 - recall: 0.6629\n",
            "Epoch 245/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5203 - accuracy: 0.7559 - precision: 0.6404 - recall: 0.6514\n",
            "Epoch 246/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5244 - accuracy: 0.7441 - precision: 0.6100 - recall: 0.6971\n",
            "Epoch 247/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.7383 - precision: 0.6020 - recall: 0.6914\n",
            "Epoch 248/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5022 - accuracy: 0.7480 - precision: 0.6223 - recall: 0.6686\n",
            "Epoch 249/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5073 - accuracy: 0.7578 - precision: 0.6321 - recall: 0.6971\n",
            "Epoch 250/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.7598 - precision: 0.6444 - recall: 0.6629\n",
            "Epoch 251/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5027 - accuracy: 0.7656 - precision: 0.6519 - recall: 0.6743\n",
            "Epoch 252/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5047 - accuracy: 0.7559 - precision: 0.6276 - recall: 0.7029\n",
            "Epoch 253/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5167 - accuracy: 0.7422 - precision: 0.6243 - recall: 0.6171\n",
            "Epoch 254/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7422 - precision: 0.6126 - recall: 0.6686\n",
            "Epoch 255/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5125 - accuracy: 0.7520 - precision: 0.6290 - recall: 0.6686\n",
            "Epoch 256/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5082 - accuracy: 0.7598 - precision: 0.6300 - recall: 0.7200\n",
            "Epoch 257/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5096 - accuracy: 0.7500 - precision: 0.6343 - recall: 0.6343\n",
            "Epoch 258/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5243 - accuracy: 0.7188 - precision: 0.5771 - recall: 0.6629\n",
            "Epoch 259/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5079 - accuracy: 0.7559 - precision: 0.6316 - recall: 0.6857\n",
            "Epoch 260/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5175 - accuracy: 0.7637 - precision: 0.6350 - recall: 0.7257\n",
            "Epoch 261/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5184 - accuracy: 0.7266 - precision: 0.6012 - recall: 0.5943\n",
            "Epoch 262/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5132 - accuracy: 0.7559 - precision: 0.6302 - recall: 0.6914\n",
            "Epoch 263/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5148 - accuracy: 0.7598 - precision: 0.6275 - recall: 0.7314\n",
            "Epoch 264/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5224 - accuracy: 0.7500 - precision: 0.6284 - recall: 0.6571\n",
            "Epoch 265/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5044 - accuracy: 0.7715 - precision: 0.6465 - recall: 0.7314\n",
            "Epoch 266/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5040 - accuracy: 0.7598 - precision: 0.6275 - recall: 0.7314\n",
            "Epoch 267/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4980 - accuracy: 0.7539 - precision: 0.6339 - recall: 0.6629\n",
            "Epoch 268/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5792 - accuracy: 0.7695 - precision: 0.6492 - recall: 0.7086\n",
            "Epoch 269/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5117 - accuracy: 0.7480 - precision: 0.6237 - recall: 0.6629\n",
            "Epoch 270/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5238 - accuracy: 0.7715 - precision: 0.6611 - recall: 0.6800\n",
            "Epoch 271/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5090 - accuracy: 0.7539 - precision: 0.6324 - recall: 0.6686\n",
            "Epoch 272/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5218 - accuracy: 0.7324 - precision: 0.6118 - recall: 0.5943\n",
            "Epoch 273/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5002 - accuracy: 0.7559 - precision: 0.6263 - recall: 0.7086\n",
            "Epoch 274/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5061 - accuracy: 0.7656 - precision: 0.6455 - recall: 0.6971\n",
            "Epoch 275/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5205 - accuracy: 0.7363 - precision: 0.6124 - recall: 0.6229\n",
            "Epoch 276/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5122 - accuracy: 0.7578 - precision: 0.6441 - recall: 0.6514\n",
            "Epoch 277/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5059 - accuracy: 0.7520 - precision: 0.6250 - recall: 0.6857\n",
            "Epoch 278/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5015 - accuracy: 0.7500 - precision: 0.6257 - recall: 0.6686\n",
            "Epoch 279/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4969 - accuracy: 0.7539 - precision: 0.6324 - recall: 0.6686\n",
            "Epoch 280/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5004 - accuracy: 0.7617 - precision: 0.6345 - recall: 0.7143\n",
            "Epoch 281/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5098 - accuracy: 0.7617 - precision: 0.6550 - recall: 0.6400\n",
            "Epoch 282/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5075 - accuracy: 0.7676 - precision: 0.6386 - recall: 0.7371\n",
            "Epoch 283/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4956 - accuracy: 0.7637 - precision: 0.6500 - recall: 0.6686\n",
            "Epoch 284/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5176 - accuracy: 0.7637 - precision: 0.6467 - recall: 0.6800\n",
            "Epoch 285/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5065 - accuracy: 0.7617 - precision: 0.6497 - recall: 0.6571\n",
            "Epoch 286/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.7637 - precision: 0.6364 - recall: 0.7200\n",
            "Epoch 287/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4991 - accuracy: 0.7695 - precision: 0.6575 - recall: 0.6800\n",
            "Epoch 288/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5164 - accuracy: 0.7520 - precision: 0.6224 - recall: 0.6971\n",
            "Epoch 289/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5125 - accuracy: 0.7461 - precision: 0.6154 - recall: 0.6857\n",
            "Epoch 290/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5089 - accuracy: 0.7480 - precision: 0.6139 - recall: 0.7086\n",
            "Epoch 291/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5048 - accuracy: 0.7578 - precision: 0.6393 - recall: 0.6686\n",
            "Epoch 292/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5079 - accuracy: 0.7539 - precision: 0.6310 - recall: 0.6743\n",
            "Epoch 293/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5147 - accuracy: 0.7695 - precision: 0.6508 - recall: 0.7029\n",
            "Epoch 294/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5053 - accuracy: 0.7285 - precision: 0.5918 - recall: 0.6629\n",
            "Epoch 295/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5139 - accuracy: 0.7676 - precision: 0.6522 - recall: 0.6857\n",
            "Epoch 296/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5315 - accuracy: 0.7461 - precision: 0.6098 - recall: 0.7143\n",
            "Epoch 297/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5096 - accuracy: 0.7656 - precision: 0.6382 - recall: 0.7257\n",
            "Epoch 298/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5076 - accuracy: 0.7422 - precision: 0.6039 - recall: 0.7143\n",
            "Epoch 299/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5281 - accuracy: 0.7422 - precision: 0.6126 - recall: 0.6686\n",
            "Epoch 300/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4948 - accuracy: 0.7520 - precision: 0.6412 - recall: 0.6229\n",
            "Epoch 301/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5002 - accuracy: 0.7617 - precision: 0.6387 - recall: 0.6971\n",
            "Epoch 302/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4977 - accuracy: 0.7715 - precision: 0.6526 - recall: 0.7086\n",
            "Epoch 303/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5075 - accuracy: 0.7637 - precision: 0.6392 - recall: 0.7086\n",
            "Epoch 304/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5055 - accuracy: 0.7793 - precision: 0.6535 - recall: 0.7543\n",
            "Epoch 305/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5220 - accuracy: 0.7422 - precision: 0.6175 - recall: 0.6457\n",
            "Epoch 306/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5112 - accuracy: 0.7598 - precision: 0.6398 - recall: 0.6800\n",
            "Epoch 307/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.7695 - precision: 0.6492 - recall: 0.7086\n",
            "Epoch 308/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5047 - accuracy: 0.7695 - precision: 0.6477 - recall: 0.7143\n",
            "Epoch 309/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5024 - accuracy: 0.7656 - precision: 0.6425 - recall: 0.7086\n",
            "Epoch 310/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4974 - accuracy: 0.7715 - precision: 0.6480 - recall: 0.7257\n",
            "Epoch 311/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5276 - accuracy: 0.7461 - precision: 0.6286 - recall: 0.6286\n",
            "Epoch 312/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7305 - precision: 0.5959 - recall: 0.6571\n",
            "Epoch 313/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4998 - accuracy: 0.7500 - precision: 0.6243 - recall: 0.6743\n",
            "Epoch 314/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4948 - accuracy: 0.7676 - precision: 0.6556 - recall: 0.6743\n",
            "Epoch 315/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5085 - accuracy: 0.7383 - precision: 0.6199 - recall: 0.6057\n",
            "Epoch 316/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5007 - accuracy: 0.7598 - precision: 0.6398 - recall: 0.6800\n",
            "Epoch 317/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5231 - accuracy: 0.7305 - precision: 0.5949 - recall: 0.6629\n",
            "Epoch 318/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5059 - accuracy: 0.7676 - precision: 0.6505 - recall: 0.6914\n",
            "Epoch 319/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5007 - accuracy: 0.7656 - precision: 0.6355 - recall: 0.7371\n",
            "Epoch 320/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5079 - accuracy: 0.7578 - precision: 0.6409 - recall: 0.6629\n",
            "Epoch 321/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4978 - accuracy: 0.7871 - precision: 0.6941 - recall: 0.6743\n",
            "Epoch 322/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5274 - accuracy: 0.7422 - precision: 0.6103 - recall: 0.6800\n",
            "Epoch 323/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5390 - accuracy: 0.7090 - precision: 0.5619 - recall: 0.6743\n",
            "Epoch 324/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5270 - accuracy: 0.7441 - precision: 0.6375 - recall: 0.5829\n",
            "Epoch 325/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5009 - accuracy: 0.7500 - precision: 0.6284 - recall: 0.6571\n",
            "Epoch 326/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5009 - accuracy: 0.7578 - precision: 0.6308 - recall: 0.7029\n",
            "Epoch 327/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5281 - accuracy: 0.7520 - precision: 0.6429 - recall: 0.6171\n",
            "Epoch 328/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5116 - accuracy: 0.7520 - precision: 0.6237 - recall: 0.6914\n",
            "Epoch 329/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5083 - accuracy: 0.7656 - precision: 0.6608 - recall: 0.6457\n",
            "Epoch 330/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5074 - accuracy: 0.7656 - precision: 0.6486 - recall: 0.6857\n",
            "Epoch 331/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4953 - accuracy: 0.7578 - precision: 0.6232 - recall: 0.7371\n",
            "Epoch 332/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4975 - accuracy: 0.7695 - precision: 0.6575 - recall: 0.6800\n",
            "Epoch 333/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4959 - accuracy: 0.7617 - precision: 0.6332 - recall: 0.7200\n",
            "Epoch 334/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5131 - accuracy: 0.7480 - precision: 0.6292 - recall: 0.6400\n",
            "Epoch 335/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4853 - accuracy: 0.7754 - precision: 0.6546 - recall: 0.7257\n",
            "Epoch 336/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5005 - accuracy: 0.7695 - precision: 0.6592 - recall: 0.6743\n",
            "Epoch 337/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5126 - accuracy: 0.7559 - precision: 0.6389 - recall: 0.6571\n",
            "Epoch 338/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.7617 - precision: 0.6359 - recall: 0.7086\n",
            "Epoch 339/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7734 - precision: 0.6686 - recall: 0.6686\n",
            "Epoch 340/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5031 - accuracy: 0.7695 - precision: 0.6557 - recall: 0.6857\n",
            "Epoch 341/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5039 - accuracy: 0.7734 - precision: 0.6528 - recall: 0.7200\n",
            "Epoch 342/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7383 - precision: 0.6096 - recall: 0.6514\n",
            "Epoch 343/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4975 - accuracy: 0.7871 - precision: 0.6602 - recall: 0.7771\n",
            "Epoch 344/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5175 - accuracy: 0.7500 - precision: 0.6313 - recall: 0.6457\n",
            "Epoch 345/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5212 - accuracy: 0.7500 - precision: 0.6257 - recall: 0.6686\n",
            "Epoch 346/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5035 - accuracy: 0.7793 - precision: 0.6598 - recall: 0.7314\n",
            "Epoch 347/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.7715 - precision: 0.6593 - recall: 0.6857\n",
            "Epoch 348/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5208 - accuracy: 0.7383 - precision: 0.6073 - recall: 0.6629\n",
            "Epoch 349/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5213 - accuracy: 0.7500 - precision: 0.6343 - recall: 0.6343\n",
            "Epoch 350/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4854 - accuracy: 0.7637 - precision: 0.6452 - recall: 0.6857\n",
            "Epoch 351/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7715 - precision: 0.6450 - recall: 0.7371\n",
            "Epoch 352/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5299 - accuracy: 0.7266 - precision: 0.6000 - recall: 0.6000\n",
            "Epoch 353/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.7500 - precision: 0.6391 - recall: 0.6171\n",
            "Epoch 354/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5088 - accuracy: 0.7461 - precision: 0.6257 - recall: 0.6400\n",
            "Epoch 355/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5057 - accuracy: 0.7461 - precision: 0.6154 - recall: 0.6857\n",
            "Epoch 356/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5006 - accuracy: 0.7734 - precision: 0.6528 - recall: 0.7200\n",
            "Epoch 357/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.7637 - precision: 0.6452 - recall: 0.6857\n",
            "Epoch 358/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4959 - accuracy: 0.7734 - precision: 0.6513 - recall: 0.7257\n",
            "Epoch 359/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5049 - accuracy: 0.7617 - precision: 0.6402 - recall: 0.6914\n",
            "Epoch 360/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.7715 - precision: 0.6559 - recall: 0.6971\n",
            "Epoch 361/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.7461 - precision: 0.6178 - recall: 0.6743\n",
            "Epoch 362/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.7734 - precision: 0.6648 - recall: 0.6800\n",
            "Epoch 363/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4989 - accuracy: 0.7715 - precision: 0.6480 - recall: 0.7257\n",
            "Epoch 364/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5160 - accuracy: 0.7637 - precision: 0.6588 - recall: 0.6400\n",
            "Epoch 365/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5120 - accuracy: 0.7461 - precision: 0.6178 - recall: 0.6743\n",
            "Epoch 366/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5050 - accuracy: 0.7617 - precision: 0.6402 - recall: 0.6914\n",
            "Epoch 367/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.7715 - precision: 0.6576 - recall: 0.6914\n",
            "Epoch 368/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4954 - accuracy: 0.7852 - precision: 0.6796 - recall: 0.7029\n",
            "Epoch 369/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4949 - accuracy: 0.7734 - precision: 0.6545 - recall: 0.7143\n",
            "Epoch 370/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4973 - accuracy: 0.7598 - precision: 0.6287 - recall: 0.7257\n",
            "Epoch 371/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4997 - accuracy: 0.7715 - precision: 0.6706 - recall: 0.6514\n",
            "Epoch 372/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5041 - accuracy: 0.7656 - precision: 0.6471 - recall: 0.6914\n",
            "Epoch 373/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5167 - accuracy: 0.7559 - precision: 0.6437 - recall: 0.6400\n",
            "Epoch 374/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5044 - accuracy: 0.7715 - precision: 0.6450 - recall: 0.7371\n",
            "Epoch 375/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4946 - accuracy: 0.7891 - precision: 0.6772 - recall: 0.7314\n",
            "Epoch 376/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5148 - accuracy: 0.7598 - precision: 0.6340 - recall: 0.7029\n",
            "Epoch 377/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4900 - accuracy: 0.7812 - precision: 0.6667 - recall: 0.7200\n",
            "Epoch 378/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.7676 - precision: 0.6522 - recall: 0.6857\n",
            "Epoch 379/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4955 - accuracy: 0.7695 - precision: 0.6462 - recall: 0.7200\n",
            "Epoch 380/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.7676 - precision: 0.6591 - recall: 0.6629\n",
            "Epoch 381/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4946 - accuracy: 0.7676 - precision: 0.6400 - recall: 0.7314\n",
            "Epoch 382/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4863 - accuracy: 0.7695 - precision: 0.6477 - recall: 0.7143\n",
            "Epoch 383/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4993 - accuracy: 0.7617 - precision: 0.6480 - recall: 0.6629\n",
            "Epoch 384/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.7520 - precision: 0.6154 - recall: 0.7314\n",
            "Epoch 385/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4886 - accuracy: 0.7871 - precision: 0.6602 - recall: 0.7771\n",
            "Epoch 386/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5003 - accuracy: 0.7480 - precision: 0.6173 - recall: 0.6914\n",
            "Epoch 387/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5106 - accuracy: 0.7676 - precision: 0.6573 - recall: 0.6686\n",
            "Epoch 388/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.7695 - precision: 0.6447 - recall: 0.7257\n",
            "Epoch 389/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5023 - accuracy: 0.7520 - precision: 0.6304 - recall: 0.6629\n",
            "Epoch 390/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5012 - accuracy: 0.7734 - precision: 0.6648 - recall: 0.6800\n",
            "Epoch 391/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.7637 - precision: 0.6337 - recall: 0.7314\n",
            "Epoch 392/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4974 - accuracy: 0.7637 - precision: 0.6500 - recall: 0.6686\n",
            "Epoch 393/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.7695 - precision: 0.6610 - recall: 0.6686\n",
            "Epoch 394/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5018 - accuracy: 0.7676 - precision: 0.6458 - recall: 0.7086\n",
            "Epoch 395/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4966 - accuracy: 0.7578 - precision: 0.6364 - recall: 0.6800\n",
            "Epoch 396/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4962 - accuracy: 0.7598 - precision: 0.6429 - recall: 0.6686\n",
            "Epoch 397/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5017 - accuracy: 0.7637 - precision: 0.6392 - recall: 0.7086\n",
            "Epoch 398/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5114 - accuracy: 0.7578 - precision: 0.6378 - recall: 0.6743\n",
            "Epoch 399/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5031 - accuracy: 0.7578 - precision: 0.6321 - recall: 0.6971\n",
            "Epoch 400/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4858 - accuracy: 0.7637 - precision: 0.6467 - recall: 0.6800\n",
            "Epoch 401/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.7695 - precision: 0.6541 - recall: 0.6914\n",
            "Epoch 402/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4991 - accuracy: 0.7871 - precision: 0.6684 - recall: 0.7486\n",
            "Epoch 403/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.7598 - precision: 0.6512 - recall: 0.6400\n",
            "Epoch 404/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5079 - accuracy: 0.7773 - precision: 0.6614 - recall: 0.7143\n",
            "Epoch 405/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4960 - accuracy: 0.7871 - precision: 0.6941 - recall: 0.6743\n",
            "Epoch 406/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4917 - accuracy: 0.7852 - precision: 0.6836 - recall: 0.6914\n",
            "Epoch 407/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5050 - accuracy: 0.7422 - precision: 0.6162 - recall: 0.6514\n",
            "Epoch 408/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5025 - accuracy: 0.7734 - precision: 0.6528 - recall: 0.7200\n",
            "Epoch 409/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4864 - accuracy: 0.7734 - precision: 0.6612 - recall: 0.6914\n",
            "Epoch 410/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.7773 - precision: 0.6564 - recall: 0.7314\n",
            "Epoch 411/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5142 - accuracy: 0.7715 - precision: 0.6495 - recall: 0.7200\n",
            "Epoch 412/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5457 - accuracy: 0.7441 - precision: 0.6341 - recall: 0.5943\n",
            "Epoch 413/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4989 - accuracy: 0.7520 - precision: 0.6395 - recall: 0.6286\n",
            "Epoch 414/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4952 - accuracy: 0.7695 - precision: 0.6447 - recall: 0.7257\n",
            "Epoch 415/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4862 - accuracy: 0.7773 - precision: 0.6649 - recall: 0.7029\n",
            "Epoch 416/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4843 - accuracy: 0.7676 - precision: 0.6556 - recall: 0.6743\n",
            "Epoch 417/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4972 - accuracy: 0.7734 - precision: 0.6766 - recall: 0.6457\n",
            "Epoch 418/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.7715 - precision: 0.6480 - recall: 0.7257\n",
            "Epoch 419/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4922 - accuracy: 0.7637 - precision: 0.6484 - recall: 0.6743\n",
            "Epoch 420/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5613 - accuracy: 0.7539 - precision: 0.6269 - recall: 0.6914\n",
            "Epoch 421/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4950 - accuracy: 0.7656 - precision: 0.6571 - recall: 0.6571\n",
            "Epoch 422/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4986 - accuracy: 0.7617 - precision: 0.6402 - recall: 0.6914\n",
            "Epoch 423/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4864 - accuracy: 0.7695 - precision: 0.6492 - recall: 0.7086\n",
            "Epoch 424/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5042 - accuracy: 0.7715 - precision: 0.6576 - recall: 0.6914\n",
            "Epoch 425/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4890 - accuracy: 0.7832 - precision: 0.6818 - recall: 0.6857\n",
            "Epoch 426/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4949 - accuracy: 0.7656 - precision: 0.6519 - recall: 0.6743\n",
            "Epoch 427/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.7754 - precision: 0.6648 - recall: 0.6914\n",
            "Epoch 428/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4943 - accuracy: 0.7773 - precision: 0.6631 - recall: 0.7086\n",
            "Epoch 429/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5116 - accuracy: 0.7578 - precision: 0.6441 - recall: 0.6514\n",
            "Epoch 430/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.7988 - precision: 0.6915 - recall: 0.7429\n",
            "Epoch 431/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4950 - accuracy: 0.7695 - precision: 0.6447 - recall: 0.7257\n",
            "Epoch 432/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5108 - accuracy: 0.7715 - precision: 0.6543 - recall: 0.7029\n",
            "Epoch 433/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4870 - accuracy: 0.7695 - precision: 0.6492 - recall: 0.7086\n",
            "Epoch 434/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4934 - accuracy: 0.7852 - precision: 0.6720 - recall: 0.7257\n",
            "Epoch 435/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4988 - accuracy: 0.7715 - precision: 0.6611 - recall: 0.6800\n",
            "Epoch 436/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.7812 - precision: 0.6649 - recall: 0.7257\n",
            "Epoch 437/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4857 - accuracy: 0.7852 - precision: 0.6738 - recall: 0.7200\n",
            "Epoch 438/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4822 - accuracy: 0.7812 - precision: 0.6684 - recall: 0.7143\n",
            "Epoch 439/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4787 - accuracy: 0.7930 - precision: 0.6825 - recall: 0.7371\n",
            "Epoch 440/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.7656 - precision: 0.6503 - recall: 0.6800\n",
            "Epoch 441/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.7715 - precision: 0.6576 - recall: 0.6914\n",
            "Epoch 442/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.7754 - precision: 0.6613 - recall: 0.7029\n",
            "Epoch 443/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4888 - accuracy: 0.7715 - precision: 0.6543 - recall: 0.7029\n",
            "Epoch 444/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.7676 - precision: 0.6538 - recall: 0.6800\n",
            "Epoch 445/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.7520 - precision: 0.6319 - recall: 0.6571\n",
            "Epoch 446/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5030 - accuracy: 0.7695 - precision: 0.6524 - recall: 0.6971\n",
            "Epoch 447/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.7891 - precision: 0.6959 - recall: 0.6800\n",
            "Epoch 448/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4937 - accuracy: 0.7422 - precision: 0.6080 - recall: 0.6914\n",
            "Epoch 449/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4850 - accuracy: 0.7891 - precision: 0.6851 - recall: 0.7086\n",
            "Epoch 450/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4868 - accuracy: 0.7891 - precision: 0.6754 - recall: 0.7371\n",
            "Epoch 451/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4969 - accuracy: 0.7637 - precision: 0.6378 - recall: 0.7143\n",
            "Epoch 452/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4893 - accuracy: 0.7637 - precision: 0.6667 - recall: 0.6171\n",
            "Epoch 453/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4858 - accuracy: 0.7832 - precision: 0.6702 - recall: 0.7200\n",
            "Epoch 454/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4941 - accuracy: 0.7695 - precision: 0.6557 - recall: 0.6857\n",
            "Epoch 455/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4875 - accuracy: 0.7773 - precision: 0.6631 - recall: 0.7086\n",
            "Epoch 456/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5000 - accuracy: 0.7695 - precision: 0.6492 - recall: 0.7086\n",
            "Epoch 457/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5043 - accuracy: 0.7598 - precision: 0.6429 - recall: 0.6686\n",
            "Epoch 458/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5019 - accuracy: 0.7695 - precision: 0.6575 - recall: 0.6800\n",
            "Epoch 459/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5477 - accuracy: 0.7422 - precision: 0.6114 - recall: 0.6743\n",
            "Epoch 460/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4944 - accuracy: 0.7480 - precision: 0.6223 - recall: 0.6686\n",
            "Epoch 461/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4861 - accuracy: 0.7871 - precision: 0.6793 - recall: 0.7143\n",
            "Epoch 462/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4791 - accuracy: 0.7793 - precision: 0.6667 - recall: 0.7086\n",
            "Epoch 463/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4877 - accuracy: 0.7656 - precision: 0.6440 - recall: 0.7029\n",
            "Epoch 464/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4904 - accuracy: 0.7695 - precision: 0.6418 - recall: 0.7371\n",
            "Epoch 465/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.7754 - precision: 0.6667 - recall: 0.6857\n",
            "Epoch 466/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5024 - accuracy: 0.7461 - precision: 0.6178 - recall: 0.6743\n",
            "Epoch 467/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4928 - accuracy: 0.7734 - precision: 0.6766 - recall: 0.6457\n",
            "Epoch 468/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5061 - accuracy: 0.7734 - precision: 0.6468 - recall: 0.7429\n",
            "Epoch 469/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4816 - accuracy: 0.7832 - precision: 0.6584 - recall: 0.7600\n",
            "Epoch 470/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.7656 - precision: 0.6440 - recall: 0.7029\n",
            "Epoch 471/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4987 - accuracy: 0.7637 - precision: 0.6452 - recall: 0.6857\n",
            "Epoch 472/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4909 - accuracy: 0.7656 - precision: 0.6396 - recall: 0.7200\n",
            "Epoch 473/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4802 - accuracy: 0.7832 - precision: 0.6778 - recall: 0.6971\n",
            "Epoch 474/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4984 - accuracy: 0.7656 - precision: 0.6382 - recall: 0.7257\n",
            "Epoch 475/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4794 - accuracy: 0.7793 - precision: 0.6598 - recall: 0.7314\n",
            "Epoch 476/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.7852 - precision: 0.6796 - recall: 0.7029\n",
            "Epoch 477/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4956 - accuracy: 0.7734 - precision: 0.6725 - recall: 0.6571\n",
            "Epoch 478/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4963 - accuracy: 0.7871 - precision: 0.6813 - recall: 0.7086\n",
            "Epoch 479/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4958 - accuracy: 0.7422 - precision: 0.6080 - recall: 0.6914\n",
            "Epoch 480/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4958 - accuracy: 0.7637 - precision: 0.6534 - recall: 0.6571\n",
            "Epoch 481/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5017 - accuracy: 0.7852 - precision: 0.6684 - recall: 0.7371\n",
            "Epoch 482/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.7578 - precision: 0.6441 - recall: 0.6514\n",
            "Epoch 483/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5008 - accuracy: 0.7617 - precision: 0.6417 - recall: 0.6857\n",
            "Epoch 484/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4907 - accuracy: 0.7754 - precision: 0.6724 - recall: 0.6686\n",
            "Epoch 485/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.7656 - precision: 0.6455 - recall: 0.6971\n",
            "Epoch 486/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4948 - accuracy: 0.7676 - precision: 0.6609 - recall: 0.6571\n",
            "Epoch 487/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4984 - accuracy: 0.7832 - precision: 0.6739 - recall: 0.7086\n",
            "Epoch 488/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4876 - accuracy: 0.7793 - precision: 0.6667 - recall: 0.7086\n",
            "Epoch 489/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5037 - accuracy: 0.7559 - precision: 0.6276 - recall: 0.7029\n",
            "Epoch 490/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.7812 - precision: 0.6684 - recall: 0.7143\n",
            "Epoch 491/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5028 - accuracy: 0.7773 - precision: 0.6763 - recall: 0.6686\n",
            "Epoch 492/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4766 - accuracy: 0.7754 - precision: 0.6648 - recall: 0.6914\n",
            "Epoch 493/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.7715 - precision: 0.6543 - recall: 0.7029\n",
            "Epoch 494/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4864 - accuracy: 0.7812 - precision: 0.6740 - recall: 0.6971\n",
            "Epoch 495/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4833 - accuracy: 0.7734 - precision: 0.6468 - recall: 0.7429\n",
            "Epoch 496/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4922 - accuracy: 0.7637 - precision: 0.6452 - recall: 0.6857\n",
            "Epoch 497/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5124 - accuracy: 0.7617 - precision: 0.6448 - recall: 0.6743\n",
            "Epoch 498/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5027 - accuracy: 0.7656 - precision: 0.6519 - recall: 0.6743\n",
            "Epoch 499/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5063 - accuracy: 0.7695 - precision: 0.6508 - recall: 0.7029\n",
            "Epoch 500/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.7891 - precision: 0.6772 - recall: 0.7314\n",
            "Epoch 501/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.7852 - precision: 0.6684 - recall: 0.7371\n",
            "Epoch 502/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4747 - accuracy: 0.7832 - precision: 0.6684 - recall: 0.7257\n",
            "Epoch 503/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4860 - accuracy: 0.7617 - precision: 0.6550 - recall: 0.6400\n",
            "Epoch 504/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4994 - accuracy: 0.7539 - precision: 0.6256 - recall: 0.6971\n",
            "Epoch 505/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4780 - accuracy: 0.7871 - precision: 0.6813 - recall: 0.7086\n",
            "Epoch 506/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4743 - accuracy: 0.7930 - precision: 0.6865 - recall: 0.7257\n",
            "Epoch 507/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.7578 - precision: 0.6378 - recall: 0.6743\n",
            "Epoch 508/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4821 - accuracy: 0.7773 - precision: 0.6763 - recall: 0.6686\n",
            "Epoch 509/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4776 - accuracy: 0.7891 - precision: 0.6683 - recall: 0.7600\n",
            "Epoch 510/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4870 - accuracy: 0.7852 - precision: 0.6702 - recall: 0.7314\n",
            "Epoch 511/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5046 - accuracy: 0.7578 - precision: 0.6294 - recall: 0.7086\n",
            "Epoch 512/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5169 - accuracy: 0.7461 - precision: 0.6178 - recall: 0.6743\n",
            "Epoch 513/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5161 - accuracy: 0.7812 - precision: 0.6886 - recall: 0.6571\n",
            "Epoch 514/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4756 - accuracy: 0.7891 - precision: 0.6754 - recall: 0.7371\n",
            "Epoch 515/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.7676 - precision: 0.6667 - recall: 0.6400\n",
            "Epoch 516/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4905 - accuracy: 0.7812 - precision: 0.6703 - recall: 0.7086\n",
            "Epoch 517/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4837 - accuracy: 0.7910 - precision: 0.6735 - recall: 0.7543\n",
            "Epoch 518/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4914 - accuracy: 0.7793 - precision: 0.6685 - recall: 0.7029\n",
            "Epoch 519/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4913 - accuracy: 0.7559 - precision: 0.6374 - recall: 0.6629\n",
            "Epoch 520/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.7930 - precision: 0.6971 - recall: 0.6971\n",
            "Epoch 521/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4963 - accuracy: 0.7676 - precision: 0.6489 - recall: 0.6971\n",
            "Epoch 522/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4863 - accuracy: 0.7676 - precision: 0.6609 - recall: 0.6571\n",
            "Epoch 523/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4944 - accuracy: 0.7832 - precision: 0.6720 - recall: 0.7143\n",
            "Epoch 524/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4803 - accuracy: 0.7637 - precision: 0.6467 - recall: 0.6800\n",
            "Epoch 525/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4934 - accuracy: 0.7832 - precision: 0.6633 - recall: 0.7429\n",
            "Epoch 526/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5010 - accuracy: 0.7754 - precision: 0.6630 - recall: 0.6971\n",
            "Epoch 527/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4964 - accuracy: 0.7812 - precision: 0.6649 - recall: 0.7257\n",
            "Epoch 528/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5133 - accuracy: 0.7461 - precision: 0.6216 - recall: 0.6571\n",
            "Epoch 529/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4740 - accuracy: 0.7715 - precision: 0.6576 - recall: 0.6914\n",
            "Epoch 530/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5383 - accuracy: 0.7812 - precision: 0.6667 - recall: 0.7200\n",
            "Epoch 531/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.7402 - precision: 0.6154 - recall: 0.6400\n",
            "Epoch 532/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.7695 - precision: 0.6541 - recall: 0.6914\n",
            "Epoch 533/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4861 - accuracy: 0.7812 - precision: 0.6649 - recall: 0.7257\n",
            "Epoch 534/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4903 - accuracy: 0.7598 - precision: 0.6461 - recall: 0.6571\n",
            "Epoch 535/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4913 - accuracy: 0.7891 - precision: 0.6936 - recall: 0.6857\n",
            "Epoch 536/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4995 - accuracy: 0.7520 - precision: 0.6277 - recall: 0.6743\n",
            "Epoch 537/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4872 - accuracy: 0.7695 - precision: 0.6575 - recall: 0.6800\n",
            "Epoch 538/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.7656 - precision: 0.6536 - recall: 0.6686\n",
            "Epoch 539/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.7734 - precision: 0.6578 - recall: 0.7029\n",
            "Epoch 540/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4721 - accuracy: 0.7832 - precision: 0.6649 - recall: 0.7371\n",
            "Epoch 541/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.7871 - precision: 0.6719 - recall: 0.7371\n",
            "Epoch 542/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4842 - accuracy: 0.7812 - precision: 0.6703 - recall: 0.7086\n",
            "Epoch 543/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5002 - accuracy: 0.7617 - precision: 0.6387 - recall: 0.6971\n",
            "Epoch 544/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4923 - accuracy: 0.7520 - precision: 0.6263 - recall: 0.6800\n",
            "Epoch 545/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4741 - accuracy: 0.7852 - precision: 0.6738 - recall: 0.7200\n",
            "Epoch 546/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4785 - accuracy: 0.7695 - precision: 0.6592 - recall: 0.6743\n",
            "Epoch 547/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4734 - accuracy: 0.7852 - precision: 0.6796 - recall: 0.7029\n",
            "Epoch 548/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4975 - accuracy: 0.7871 - precision: 0.6755 - recall: 0.7257\n",
            "Epoch 549/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4769 - accuracy: 0.7871 - precision: 0.6813 - recall: 0.7086\n",
            "Epoch 550/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5064 - accuracy: 0.7695 - precision: 0.6557 - recall: 0.6857\n",
            "Epoch 551/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4861 - accuracy: 0.7793 - precision: 0.6742 - recall: 0.6857\n",
            "Epoch 552/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4796 - accuracy: 0.7695 - precision: 0.6647 - recall: 0.6571\n",
            "Epoch 553/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4845 - accuracy: 0.7676 - precision: 0.6573 - recall: 0.6686\n",
            "Epoch 554/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4824 - accuracy: 0.7949 - precision: 0.7035 - recall: 0.6914\n",
            "Epoch 555/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4958 - accuracy: 0.7793 - precision: 0.6722 - recall: 0.6914\n",
            "Epoch 556/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4740 - accuracy: 0.7852 - precision: 0.6946 - recall: 0.6629\n",
            "Epoch 557/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4866 - accuracy: 0.7852 - precision: 0.6667 - recall: 0.7429\n",
            "Epoch 558/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4779 - accuracy: 0.7715 - precision: 0.6510 - recall: 0.7143\n",
            "Epoch 559/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4744 - accuracy: 0.7969 - precision: 0.6859 - recall: 0.7486\n",
            "Epoch 560/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4830 - accuracy: 0.7637 - precision: 0.6607 - recall: 0.6343\n",
            "Epoch 561/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.7754 - precision: 0.6531 - recall: 0.7314\n",
            "Epoch 562/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4906 - accuracy: 0.7812 - precision: 0.6649 - recall: 0.7257\n",
            "Epoch 563/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4777 - accuracy: 0.7793 - precision: 0.6550 - recall: 0.7486\n",
            "Epoch 564/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.7734 - precision: 0.6482 - recall: 0.7371\n",
            "Epoch 565/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4752 - accuracy: 0.7852 - precision: 0.6836 - recall: 0.6914\n",
            "Epoch 566/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4916 - accuracy: 0.7656 - precision: 0.6440 - recall: 0.7029\n",
            "Epoch 567/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.7754 - precision: 0.6829 - recall: 0.6400\n",
            "Epoch 568/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4962 - accuracy: 0.7871 - precision: 0.6854 - recall: 0.6971\n",
            "Epoch 569/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5005 - accuracy: 0.7598 - precision: 0.6354 - recall: 0.6971\n",
            "Epoch 570/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4973 - accuracy: 0.7637 - precision: 0.6452 - recall: 0.6857\n",
            "Epoch 571/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4803 - accuracy: 0.7715 - precision: 0.6465 - recall: 0.7314\n",
            "Epoch 572/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4828 - accuracy: 0.7637 - precision: 0.6534 - recall: 0.6571\n",
            "Epoch 573/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4747 - accuracy: 0.7812 - precision: 0.6721 - recall: 0.7029\n",
            "Epoch 574/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4898 - accuracy: 0.7773 - precision: 0.6685 - recall: 0.6914\n",
            "Epoch 575/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5073 - accuracy: 0.7695 - precision: 0.6524 - recall: 0.6971\n",
            "Epoch 576/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4845 - accuracy: 0.7871 - precision: 0.6774 - recall: 0.7200\n",
            "Epoch 577/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4825 - accuracy: 0.7715 - precision: 0.6648 - recall: 0.6686\n",
            "Epoch 578/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4801 - accuracy: 0.7910 - precision: 0.6700 - recall: 0.7657\n",
            "Epoch 579/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4841 - accuracy: 0.7656 - precision: 0.6440 - recall: 0.7029\n",
            "Epoch 580/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.7773 - precision: 0.6826 - recall: 0.6514\n",
            "Epoch 581/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.5231 - accuracy: 0.7578 - precision: 0.6281 - recall: 0.7143\n",
            "Epoch 582/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.7871 - precision: 0.6774 - recall: 0.7200\n",
            "Epoch 583/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4732 - accuracy: 0.7910 - precision: 0.6828 - recall: 0.7257\n",
            "Epoch 584/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5570 - accuracy: 0.7637 - precision: 0.6452 - recall: 0.6857\n",
            "Epoch 585/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5071 - accuracy: 0.7617 - precision: 0.6568 - recall: 0.6343\n",
            "Epoch 586/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5054 - accuracy: 0.7539 - precision: 0.6269 - recall: 0.6914\n",
            "Epoch 587/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4912 - accuracy: 0.7656 - precision: 0.6303 - recall: 0.7600\n",
            "Epoch 588/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4822 - accuracy: 0.7773 - precision: 0.6667 - recall: 0.6971\n",
            "Epoch 589/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4729 - accuracy: 0.7930 - precision: 0.6845 - recall: 0.7314\n",
            "Epoch 590/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4757 - accuracy: 0.7793 - precision: 0.6867 - recall: 0.6514\n",
            "Epoch 591/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.7695 - precision: 0.6462 - recall: 0.7200\n",
            "Epoch 592/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5010 - accuracy: 0.7715 - precision: 0.6559 - recall: 0.6971\n",
            "Epoch 593/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4758 - accuracy: 0.7832 - precision: 0.6720 - recall: 0.7143\n",
            "Epoch 594/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4833 - accuracy: 0.7793 - precision: 0.6722 - recall: 0.6914\n",
            "Epoch 595/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4776 - accuracy: 0.7871 - precision: 0.6684 - recall: 0.7486\n",
            "Epoch 596/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4744 - accuracy: 0.8008 - precision: 0.6973 - recall: 0.7371\n",
            "Epoch 597/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4811 - accuracy: 0.7676 - precision: 0.6443 - recall: 0.7143\n",
            "Epoch 598/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4702 - accuracy: 0.7852 - precision: 0.6796 - recall: 0.7029\n",
            "Epoch 599/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4838 - accuracy: 0.7695 - precision: 0.6541 - recall: 0.6914\n",
            "Epoch 600/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4753 - accuracy: 0.7949 - precision: 0.6966 - recall: 0.7086\n",
            "Epoch 601/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4761 - accuracy: 0.8008 - precision: 0.6952 - recall: 0.7429\n",
            "Epoch 602/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4697 - accuracy: 0.7773 - precision: 0.6667 - recall: 0.6971\n",
            "Epoch 603/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4953 - accuracy: 0.7773 - precision: 0.6597 - recall: 0.7200\n",
            "Epoch 604/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4960 - accuracy: 0.7598 - precision: 0.6477 - recall: 0.6514\n",
            "Epoch 605/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5019 - accuracy: 0.7871 - precision: 0.6854 - recall: 0.6971\n",
            "Epoch 606/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4896 - accuracy: 0.7656 - precision: 0.6471 - recall: 0.6914\n",
            "Epoch 607/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.7461 - precision: 0.6243 - recall: 0.6457\n",
            "Epoch 608/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.7754 - precision: 0.6596 - recall: 0.7086\n",
            "Epoch 609/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4838 - accuracy: 0.7637 - precision: 0.6406 - recall: 0.7029\n",
            "Epoch 610/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4852 - accuracy: 0.7773 - precision: 0.6597 - recall: 0.7200\n",
            "Epoch 611/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4770 - accuracy: 0.7891 - precision: 0.6831 - recall: 0.7143\n",
            "Epoch 612/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4857 - accuracy: 0.7637 - precision: 0.6452 - recall: 0.6857\n",
            "Epoch 613/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4869 - accuracy: 0.7637 - precision: 0.6646 - recall: 0.6229\n",
            "Epoch 614/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4986 - accuracy: 0.7598 - precision: 0.6429 - recall: 0.6686\n",
            "Epoch 615/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4713 - accuracy: 0.7910 - precision: 0.6753 - recall: 0.7486\n",
            "Epoch 616/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4832 - accuracy: 0.7793 - precision: 0.6761 - recall: 0.6800\n",
            "Epoch 617/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.7559 - precision: 0.6276 - recall: 0.7029\n",
            "Epoch 618/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.7754 - precision: 0.6613 - recall: 0.7029\n",
            "Epoch 619/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.7695 - precision: 0.6492 - recall: 0.7086\n",
            "Epoch 620/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4717 - accuracy: 0.7852 - precision: 0.6702 - recall: 0.7314\n",
            "Epoch 621/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4660 - accuracy: 0.7949 - precision: 0.6944 - recall: 0.7143\n",
            "Epoch 622/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4786 - accuracy: 0.7910 - precision: 0.6848 - recall: 0.7200\n",
            "Epoch 623/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4730 - accuracy: 0.8047 - precision: 0.7027 - recall: 0.7429\n",
            "Epoch 624/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.7969 - precision: 0.7029 - recall: 0.7029\n",
            "Epoch 625/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4750 - accuracy: 0.7559 - precision: 0.6359 - recall: 0.6686\n",
            "Epoch 626/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.7891 - precision: 0.6736 - recall: 0.7429\n",
            "Epoch 627/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4896 - accuracy: 0.7734 - precision: 0.6667 - recall: 0.6743\n",
            "Epoch 628/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4681 - accuracy: 0.7832 - precision: 0.6684 - recall: 0.7257\n",
            "Epoch 629/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4766 - accuracy: 0.7910 - precision: 0.6809 - recall: 0.7314\n",
            "Epoch 630/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4822 - accuracy: 0.7812 - precision: 0.6649 - recall: 0.7257\n",
            "Epoch 631/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4729 - accuracy: 0.7773 - precision: 0.6631 - recall: 0.7086\n",
            "Epoch 632/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4720 - accuracy: 0.7832 - precision: 0.6778 - recall: 0.6971\n",
            "Epoch 633/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4697 - accuracy: 0.7656 - precision: 0.6486 - recall: 0.6857\n",
            "Epoch 634/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.7754 - precision: 0.6613 - recall: 0.7029\n",
            "Epoch 635/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4688 - accuracy: 0.7891 - precision: 0.6959 - recall: 0.6800\n",
            "Epoch 636/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4887 - accuracy: 0.7695 - precision: 0.6492 - recall: 0.7086\n",
            "Epoch 637/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.7695 - precision: 0.6492 - recall: 0.7086\n",
            "Epoch 638/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5702 - accuracy: 0.7441 - precision: 0.6294 - recall: 0.6114\n",
            "Epoch 639/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4848 - accuracy: 0.7715 - precision: 0.6629 - recall: 0.6743\n",
            "Epoch 640/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4758 - accuracy: 0.7832 - precision: 0.6905 - recall: 0.6629\n",
            "Epoch 641/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.7852 - precision: 0.6776 - recall: 0.7086\n",
            "Epoch 642/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4734 - accuracy: 0.7969 - precision: 0.6983 - recall: 0.7143\n",
            "Epoch 643/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4746 - accuracy: 0.7852 - precision: 0.6857 - recall: 0.6857\n",
            "Epoch 644/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.7832 - precision: 0.6720 - recall: 0.7143\n",
            "Epoch 645/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4981 - accuracy: 0.7500 - precision: 0.6181 - recall: 0.7029\n",
            "Epoch 646/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4847 - accuracy: 0.7695 - precision: 0.6647 - recall: 0.6571\n",
            "Epoch 647/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4780 - accuracy: 0.7812 - precision: 0.6632 - recall: 0.7314\n",
            "Epoch 648/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.7891 - precision: 0.6791 - recall: 0.7257\n",
            "Epoch 649/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4793 - accuracy: 0.7734 - precision: 0.6667 - recall: 0.6743\n",
            "Epoch 650/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4745 - accuracy: 0.7754 - precision: 0.6630 - recall: 0.6971\n",
            "Epoch 651/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4837 - accuracy: 0.7891 - precision: 0.6736 - recall: 0.7429\n",
            "Epoch 652/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4830 - accuracy: 0.7656 - precision: 0.6425 - recall: 0.7086\n",
            "Epoch 653/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4730 - accuracy: 0.7988 - precision: 0.7022 - recall: 0.7143\n",
            "Epoch 654/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4749 - accuracy: 0.7910 - precision: 0.6771 - recall: 0.7429\n",
            "Epoch 655/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5030 - accuracy: 0.7773 - precision: 0.6685 - recall: 0.6914\n",
            "Epoch 656/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4750 - accuracy: 0.7773 - precision: 0.6763 - recall: 0.6686\n",
            "Epoch 657/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4918 - accuracy: 0.7480 - precision: 0.6223 - recall: 0.6686\n",
            "Epoch 658/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4746 - accuracy: 0.7793 - precision: 0.6703 - recall: 0.6971\n",
            "Epoch 659/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4844 - accuracy: 0.7793 - precision: 0.6722 - recall: 0.6914\n",
            "Epoch 660/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4865 - accuracy: 0.7871 - precision: 0.6684 - recall: 0.7486\n",
            "Epoch 661/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4836 - accuracy: 0.7734 - precision: 0.6686 - recall: 0.6686\n",
            "Epoch 662/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4712 - accuracy: 0.8008 - precision: 0.6891 - recall: 0.7600\n",
            "Epoch 663/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5083 - accuracy: 0.7715 - precision: 0.6450 - recall: 0.7371\n",
            "Epoch 664/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4938 - accuracy: 0.7656 - precision: 0.6519 - recall: 0.6743\n",
            "Epoch 665/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4678 - accuracy: 0.7910 - precision: 0.6954 - recall: 0.6914\n",
            "Epoch 666/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4724 - accuracy: 0.7695 - precision: 0.6541 - recall: 0.6914\n",
            "Epoch 667/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4769 - accuracy: 0.7754 - precision: 0.6648 - recall: 0.6914\n",
            "Epoch 668/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4762 - accuracy: 0.7695 - precision: 0.6629 - recall: 0.6629\n",
            "Epoch 669/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4621 - accuracy: 0.7988 - precision: 0.6856 - recall: 0.7600\n",
            "Epoch 670/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4823 - accuracy: 0.7832 - precision: 0.6905 - recall: 0.6629\n",
            "Epoch 671/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4717 - accuracy: 0.8027 - precision: 0.6907 - recall: 0.7657\n",
            "Epoch 672/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4843 - accuracy: 0.7832 - precision: 0.6758 - recall: 0.7029\n",
            "Epoch 673/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4942 - accuracy: 0.7617 - precision: 0.6587 - recall: 0.6286\n",
            "Epoch 674/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4691 - accuracy: 0.7910 - precision: 0.6789 - recall: 0.7371\n",
            "Epoch 675/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.7734 - precision: 0.6746 - recall: 0.6514\n",
            "Epoch 676/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4765 - accuracy: 0.7617 - precision: 0.6402 - recall: 0.6914\n",
            "Epoch 677/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4899 - accuracy: 0.7832 - precision: 0.6818 - recall: 0.6857\n",
            "Epoch 678/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7637 - precision: 0.6467 - recall: 0.6800\n",
            "Epoch 679/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4914 - accuracy: 0.7656 - precision: 0.6471 - recall: 0.6914\n",
            "Epoch 680/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4781 - accuracy: 0.7812 - precision: 0.6632 - recall: 0.7314\n",
            "Epoch 681/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4748 - accuracy: 0.7852 - precision: 0.6702 - recall: 0.7314\n",
            "Epoch 682/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4674 - accuracy: 0.7891 - precision: 0.6914 - recall: 0.6914\n",
            "Epoch 683/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.7871 - precision: 0.6793 - recall: 0.7143\n",
            "Epoch 684/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4834 - accuracy: 0.7793 - precision: 0.6782 - recall: 0.6743\n",
            "Epoch 685/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4871 - accuracy: 0.7793 - precision: 0.6667 - recall: 0.7086\n",
            "Epoch 686/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4664 - accuracy: 0.7773 - precision: 0.6763 - recall: 0.6686\n",
            "Epoch 687/700\n",
            "52/52 [==============================] - 0s 1ms/step - loss: 0.4733 - accuracy: 0.7949 - precision: 0.6786 - recall: 0.7600\n",
            "Epoch 688/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4702 - accuracy: 0.7852 - precision: 0.6901 - recall: 0.6743\n",
            "Epoch 689/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4702 - accuracy: 0.7734 - precision: 0.6513 - recall: 0.7257\n",
            "Epoch 690/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4746 - accuracy: 0.7910 - precision: 0.6868 - recall: 0.7143\n",
            "Epoch 691/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4930 - accuracy: 0.7832 - precision: 0.6758 - recall: 0.7029\n",
            "Epoch 692/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4741 - accuracy: 0.7715 - precision: 0.6495 - recall: 0.7200\n",
            "Epoch 693/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4709 - accuracy: 0.7754 - precision: 0.6786 - recall: 0.6514\n",
            "Epoch 694/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4855 - accuracy: 0.7773 - precision: 0.6763 - recall: 0.6686\n",
            "Epoch 695/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4901 - accuracy: 0.7715 - precision: 0.6648 - recall: 0.6686\n",
            "Epoch 696/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4667 - accuracy: 0.7773 - precision: 0.6564 - recall: 0.7314\n",
            "Epoch 697/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4758 - accuracy: 0.7910 - precision: 0.6932 - recall: 0.6971\n",
            "Epoch 698/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4948 - accuracy: 0.7773 - precision: 0.6564 - recall: 0.7314\n",
            "Epoch 699/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.5199 - accuracy: 0.7617 - precision: 0.6417 - recall: 0.6857\n",
            "Epoch 700/700\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.7656 - precision: 0.6410 - recall: 0.7143\n",
            "52/52 [==============================] - 0s 2ms/step - loss: 0.4826 - accuracy: 0.7715 - precision: 0.7458 - recall: 0.5029\n",
            "Train result:\n",
            "Batch 1 accuracy: 77%%\n",
            "Batch 1 precision: 75%%\n",
            "Batch 1 recall: 50%%\n",
            "26/26 [==============================] - 0s 2ms/step - loss: 0.5315 - accuracy: 0.7500 - precision: 0.6986 - recall: 0.5484\n",
            "Test result:\n",
            "Batch 1 accuracy: 75%%\n",
            "Batch 1 precision: 70%%\n",
            "Batch 1 recall: 55%%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-108-b435331b18b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mplot_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMETRICS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-107-26cc7864e772>\u001b[0m in \u001b[0;36mplot_curve\u001b[0;34m(epochs, hist, list_of_metrics)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2964\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2965\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2966\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mapply_if_callable\u001b[0;34m(maybe_callable, obj, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m  \u001b[0;31m# pylint:disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     return distributed_training_utils.call_replica_local_fn(\n\u001b[0;32m--> 208\u001b[0;31m         replica_local_fn, *args, **kwargs)\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/distribute/distributed_training_utils.py\u001b[0m in \u001b[0;36mcall_replica_local_fn\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mreplica_local_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplica_local_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m       \u001b[0;34m\"\"\"Updates the state of the metric in a replica-local context.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m       \u001b[0mupdate_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/metrics_utils.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_context_for_symbolic_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m       \u001b[0mupdate_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_state_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mupdate_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# update_op will be None in eager execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0mmetric_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: update_state() missing 1 required positional argument: 'y_pred'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpS9Aq1t8TAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # The following variables are the hyperparameters.\n",
        "# learning_rate = 0.001\n",
        "# epochs = 700\n",
        "# batch_size = 10\n",
        "# label_name = \"Outcome\"\n",
        "# classification_threshold = 0.4\n",
        "\n",
        "# # Establish the metrics the model will measure.\n",
        "# METRICS = [\n",
        "#            tf.keras.metrics.BinaryAccuracy(name='accuracy', \n",
        "#                                            threshold=classification_threshold),\n",
        "#            tf.keras.metrics.Precision(thresholds=classification_threshold,\n",
        "#                                  name='precision' \n",
        "#                                  ),\n",
        "#            tf.keras.metrics.Recall(thresholds=classification_threshold,\n",
        "#                                  name='recall' \n",
        "#                                  ),\n",
        "#           ]\n",
        "\n",
        "# # Establish the model's topography.\n",
        "# my_model = create_model(learning_rate, feature_layer, METRICS)\n",
        "\n",
        "# # Train the model on the training set.\n",
        "# epochs, hist = train_model(my_model, X_train, epochs, \n",
        "#                            label_name, batch_size)\n",
        "\n",
        "# # Plot a graph of the metric(s) vs. epochs.\n",
        "# list_of_metrics_to_plot = ['accuracy', 'precision', 'recall'] \n",
        "\n",
        "# plot_curve(epochs, hist, list_of_metrics_to_plot)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}